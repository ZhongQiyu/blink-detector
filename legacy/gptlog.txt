hey gpt, are there any other ways to detect ai user blinking from a camera other than mediapipe or dlib?
ChatGPT
Yes, there are other methods and technologies available for detecting eye blinking from a camera beyond MediaPipe or Dlib. Here are some alternatives:

OpenCV Haar Cascades: OpenCV provides Haar Cascade classifiers that can be used for facial feature detection, including eyes. Once the eyes are detected, you can analyze the opening and closing of eyes to detect blinking.

Custom Deep Learning Models: You can train a deep learning model using convolutional neural networks (CNNs) to specifically recognize eye blinking. This requires a dataset of images or videos labeled with blinking and non-blinking states.

Traditional Computer Vision Techniques: Techniques like edge detection, blob detection, or thresholding can be used to analyze the eye region and detect blinking. This approach might be less accurate than machine learning methods but can be simpler to implement in certain cases.

Pre-trained Models from Deep Learning Frameworks: Frameworks like TensorFlow or PyTorch offer pre-trained models that can be fine-tuned for specific tasks like blinking detection.

Third-Party APIs and SDKs: There are several APIs and SDKs provided by various companies that offer facial feature recognition, including eye blinking detection. These are often easy to integrate but may come with usage costs or limitations.

Eye Aspect Ratio (EAR) Algorithm: This algorithm calculates the aspect ratio of the eye and uses it to determine whether the eye is open or closed. A sudden change in this ratio can indicate a blink.

Infrared or 3D Cameras: Using specialized cameras like infrared or 3D depth cameras can provide more accurate detection of blinking, especially in low-light conditions or for more complex analyses.

It's important to choose a method that aligns with your specific requirements, such as accuracy, real-time processing needs, lighting conditions, and hardware availability.

User
ok, here is code for my python app: 
import time
import threading
import cv2
import mediapipe as mp
import tkinter as tk
from PIL import Image, ImageTk
from pynput.mouse import Listener as MouseListener
from pynput.keyboard import Listener as KeyboardListener, Key
from tkinter import ttk



class EyeTrackingApp:
    def __init__(self, window_title):
        self.root = tk.Tk()
        self.root.state('zoomed')
        self.root.title(window_title)
        self.root.configure(bg='#404040')

        self.on_break = False

        # Create a style object
        style = ttk.Style()
        style.theme_use('clam')  # Use the 'clam' theme as a base for customization

        # Configure style for Button
        style.configure('Custom.TButton', 
                        background='#404040', 
                        foreground='white', 
                        bordercolor='white', 
                        borderwidth=2,
                        font=('Segoe UI', 14))

        # Configure style for Text
        text_bg = '#404040'  # Dark grey background for text
        text_fg = 'white'    # White text color

        # Main Frame
        main_frame = tk.Frame(self.root, bg='#404040')
        main_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Video Frame
        video_frame = tk.Frame(main_frame, bg='#404040')
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # Right Section Frame
        right_frame = tk.Frame(main_frame, bg='#404040')
        right_frame.pack(side=tk.RIGHT, fill=tk.Y)

        self.canvas_video = tk.Canvas(video_frame, bg='#404040')
        self.canvas_video.pack(fill="both", expand=True)

        # Time Elapsed Frame
        self.total_time_count = 0
        time_frame = tk.Frame(right_frame, bg='#404040')
        time_frame.pack(side=tk.TOP, fill=tk.X, pady=5)
        self.total_time_elapsed = tk.Label(time_frame, text="Total Time Elapsed: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_time_elapsed.pack()
        self.change_total_time_count()

        # Strictness Frame
        self.strictness = 10
        strictness_frame = tk.Frame(right_frame, bg='#404040')
        strictness_frame.pack(side=tk.TOP, fill=tk.X)
        strictness_frame.columnconfigure(0, weight=1)
        strictness_frame.columnconfigure(1, weight=1)
        self.strictness_value = tk.Label(strictness_frame, font=("Segoe UI", 20), text='Blink Strictness: ' + str(self.strictness), fg='white', bg='#404040')
        self.strictness_value.grid(row=0, columnspan=2)
        self.strictness_textbox = tk.Text(strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_strictness_button = ttk.Button(strictness_frame, text="Set Strictness", style='Custom.TButton', command=self.set_strictness)
        self.set_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.strictness_explanation = tk.Label(strictness_frame, text="Blink Strictness means the maximum blink count per minute", font=("Arial", 15), fg='white', bg='#404040')
        self.strictness_explanation.grid(row=3, columnspan=2)
        self.warning_msg = tk.Label(strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.warning_msg.grid(row=2, columnspan=2)

        # Blink Count Frame
        self.blink_count = 0
        self.EAR_THRESHOLD = 0.21
        self.eye_closed = False
        self.blink_count_frame = tk.Frame(right_frame, bg='#404040')
        self.blink_count_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_blink_count = tk.Label(self.blink_count_frame, text="Total Blink Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_blink_count.pack()

        # Break Label
        self.break_label = tk.Label(right_frame, text="Time for a break!", font=("Segoe UI", 20), fg='red', bg='#404040')
        self.break_label.pack(pady=10)
        self.break_label.pack_forget()

        # Spacer Frame
        spacer_frame = tk.Frame(right_frame, height=20, bg='#404040')
        spacer_frame.pack(side=tk.TOP, fill=tk.X)

        # Clicks and Keystrokes Frame
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        clicks_frame = tk.Frame(right_frame, bg='#404040')
        clicks_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_clicks = tk.Label(clicks_frame, text="Total Clicks: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_clicks.pack()
        keystrokes_frame = tk.Frame(right_frame, bg='#404040')
        keystrokes_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_keystrokes_label = tk.Label(keystrokes_frame, text="Total Keystroke Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_keystrokes_label.pack()

        # Total Inputs Frame
        total_inputs_frame = tk.Frame(right_frame, bg='#404040')
        total_inputs_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_inputs_label = tk.Label(total_inputs_frame, text="Total Inputs: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_inputs_label.pack()

        # Input Strictness Frame
        self.input_strictness = 50
        input_strictness_frame = tk.Frame(right_frame, bg='#404040')
        input_strictness_frame.pack(side=tk.TOP, fill=tk.X)
        input_strictness_frame.columnconfigure(0, weight=1)
        input_strictness_frame.columnconfigure(1, weight=1)
        self.input_strictness_value = tk.Label(input_strictness_frame, font=("Segoe UI", 20), text='Input Strictness: ' + str(self.input_strictness), fg='white', bg='#404040')
        self.input_strictness_value.grid(row=0, columnspan=2)
        self.input_strictness_textbox = tk.Text(input_strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.input_strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_input_strictness_button = ttk.Button(input_strictness_frame, text="Set Input Strictness", style='Custom.TButton', command=self.set_input_strictness)
        self.set_input_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.input_strictness_warning_msg = tk.Label(input_strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.input_strictness_warning_msg.grid(row=2, columnspan=2)

        # Reset Countdown Label
        self.reset_countdown_label = tk.Label(right_frame, text="Resets in 60 seconds", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.reset_countdown_label.pack(side=tk.TOP, fill=tk.X)

        # Input Listener Thread
        input_listener_thread = threading.Thread(target=self.run_input_listeners)
        input_listener_thread.daemon = True
        input_listener_thread.start()

        # Initialize attributes for video capture and face mesh
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)
        self.is_tracking = False
        self.video_source = 0
        self.vid = cv2.VideoCapture(self.video_source)
        self.delay = 15

        # Start video updates
        self.root.after(100, self.start_updates)
        self.handle_reset_countdown()
        self.root.mainloop()
    def set_input_strictness(self):
        value = self.input_strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 500:
                self.input_strictness = intvalue
                self.input_strictness_value.config(text="Input Strictness: " + str(intvalue))
                self.input_strictness_warning_msg.config(text="")
            else:
                self.input_strictness_warning_msg.config(text="Invalid Input! Please enter a number between 0 and 500!")
        except ValueError:
            self.input_strictness_warning_msg.config(text="Invalid Input! Please enter an integer!")

    def reset_counters(self):
        # Reset all the counters and update their respective labels
        self.blink_count = 0
        self.total_blink_count.config(text="Total Blink Count: 0")
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        self.update_click_count()
        self.update_keystroke_count()
        self.update_total_inputs_label()

    def change_total_time_count(self):
        self.total_time_elapsed.config(text=f"Total Time Elapsed: {self.total_time_count}")
        self.total_time_count += 1
        self.root.after(1000, self.change_total_time_count)

    def update_total_inputs_label(self):
        total_inputs = self.total_click_amount + self.total_keystroke_count
        self.total_inputs_label.config(text=f"Total Inputs: {total_inputs}")
        if total_inputs >= self.input_strictness and not self.on_break:
            self.initiate_break()


    def show_break_label(self):
        self.break_label.pack()

    def hide_break_label(self):
        self.break_label.pack_forget()

    def run_input_listeners(self):
        # Initialize and start the mouse listener
        mouse_listener = MouseListener(on_click=self.on_click)
        mouse_listener.start()

        # Initialize and start the keyboard listener
        keyboard_listener = KeyboardListener(on_press=self.on_press)
        keyboard_listener.start()

        # Keep the thread running
        mouse_listener.join()
        keyboard_listener.join()


    def start_eye_tracking(self):
        # Directly start tracking without checking the button state
        self.is_tracking = True
        self.canvas_video.configure(bg='black')

    def handle_reset_countdown(self, countdown=60):
        if hasattr(self, 'after_id'):  # Check if there's an existing after call
            self.root.after_cancel(self.after_id)  # Cancel the previous after call

        if self.on_break:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Break time! {countdown} seconds remaining")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.on_break = False
                self.reset_countdown_label.config(text="Resets in 60 seconds")
                self.hide_break_label()
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown
        else:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Resets in {countdown} seconds")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown


    

    def clear_video_feed(self):
        self.canvas_video.delete("all")
        self.canvas_video.configure(bg='white')
        self.canvas_video.create_text(
            self.canvas_video.winfo_width() // 2, self.canvas_video.winfo_height() // 2,
            text="Video feed stopped", font=("Arial", 20), fill="black"
        )

    def start_updates(self):
        if not self.is_tracking:
            self.start_eye_tracking()
        self.update()

    def update(self):
        ret, frame = self.vid.read()
        if ret and self.is_tracking:
            frame = cv2.flip(frame, 1)
            canvas_width = self.canvas_video.winfo_width()
            canvas_height = self.canvas_video.winfo_height()

            if canvas_width > 0 and canvas_height > 0:
                frame = self.resize_with_aspect_ratio(frame, width=canvas_width, height=canvas_height)
                if frame is not None:
                    frame = self.detect_eyes(frame)
                    self.photo = ImageTk.PhotoImage(image=Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                    self.canvas_video.create_image((canvas_width - self.photo.width()) // 2, (canvas_height - self.photo.height()) // 2, image=self.photo, anchor=tk.NW)
        self.root.after(self.delay, self.update)


    def resize_with_aspect_ratio(self, image, width=None, height=None, inter=cv2.INTER_AREA):
        (h, w) = image.shape[:2]

        # Calculate the aspect ratio of the image and the desired aspect ratio
        image_aspect = w / h
        desired_aspect = width / height if width is not None and height is not None else image_aspect

        # Calculate scaling factors for resizing the image while maintaining aspect ratio
        if image_aspect > desired_aspect:
            # Image is wider than the desired aspect ratio
            r = width / float(w)
            dim = (width, int(h * r))
        else:
            # Image is taller or equal to the desired aspect ratio
            r = height / float(h)
            dim = (int(w * r), height)

        # Check that dimensions are valid before attempting to resize
        if dim[0] > 0 and dim[1] > 0:
            resized = cv2.resize(image, dim, interpolation=inter)
            return resized
        return None


    def detect_eyes(self, frame):
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(image)
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                eye_indices = list(range(130, 160)) + list(range(385, 398))
                for idx in eye_indices:
                    if idx < len(face_landmarks.landmark):
                        point = face_landmarks.landmark[idx]
                        x = int(point.x * frame.shape[1])
                        y = int(point.y * frame.shape[0])
                        cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)
        return frame

    def set_strictness(self):
        value = self.strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 75:
                self.strictness = intvalue
                self.strictness_value.config(text=" Blink Strictness: " + str(intvalue))
                self.warning_msg.config(text="")
            else:
                self.warning_msg.config(text="Invalid Input! Please enter a number between 0 and 75!")
        except ValueError:
            self.warning_msg.config(text="Invalid Input! Please enter an integer!")

    def on_click(self, x, y, button, pressed):
        if pressed and not self.on_break:
            self.total_click_amount += 1
            self.update_click_count()
            self.update_total_inputs_label()

    def on_press(self, key):
        if not self.on_break:
            self.total_keystroke_count += 1
            self.update_keystroke_count()
            self.update_total_inputs_label()


    def update_click_count(self):
        self.total_clicks.config(text="Total Clicks: " + str(self.total_click_amount))
        self.update_total_inputs_label()

    def update_keystroke_count(self):
        self.total_keystrokes_label.config(text="Total Keystroke Count: " + str(self.total_keystroke_count))
    def run_mouse_listener(self):
        with MouseListener(on_click=self.on_click) as listener:
            listener.join()

    # Separate method to run the keyboard listener
    def run_keyboard_listener(self):
        with KeyboardListener(on_press=self.on_press) as listener:
            listener.join()

    """
        Eye blink engine time!
    """

    def update_blink_count(self):
        if not self.on_break:  # Only update blink count if not on a break
            self.total_blink_count.config(text=f"Total Blink Count: {self.blink_count}")

            if self.blink_count >= self.strictness:
                self.initiate_break()

    def initiate_break(self):
        # Set the state to break and update the UI accordingly
        self.on_break = True
        self.show_break_label()
        self.reset_countdown_label.config(text="Timer stopped, take a break!")
        self.handle_reset_countdown(30)  # Start a 30-second break countdown

        # Reset all counts
        self.reset_counters()

    def detect_eyes(self, frame):
        # Convert the frame to RGB for MediaPipe processing
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(image)

        # Check if any face landmarks were detected
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Hypothetical indices for the landmarks of each eye.
                # You need to replace these with the correct indices from MediaPipe
                right_eye_indices = [33, 160, 158, 133, 153, 144]  # replace with the correct indices
                left_eye_indices = [362, 385, 387, 263, 373, 380]

                # Left eye indices list
                """
                LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]
                # Right eye indices list
                RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]
                """

                # Extract the landmark coordinates for each eye
                right_eye = [face_landmarks.landmark[i] for i in right_eye_indices]
                left_eye = [face_landmarks.landmark[i] for i in left_eye_indices]

                # Calculate EAR for both eyes
                left_ear = self.calculate_ear(left_eye)
                right_ear = self.calculate_ear(right_eye)

                # Average the EAR for both eyes for better stability
                ear = (left_ear + right_ear) / 2.0

                # Blink detection logic
                if self.eye_closed and ear > self.EAR_THRESHOLD:
                    self.blink_count += 1
                    self.update_blink_count()
                    self.eye_closed = False  # Reset the eye closed flag
                elif not self.eye_closed and ear < self.EAR_THRESHOLD:
                    self.eye_closed = True  # Set the eye closed flag

                # Visualize the eye landmarks for debugging purposes
                for idx in right_eye_indices + left_eye_indices:
                    if idx < len(face_landmarks.landmark):
                        point = face_landmarks.landmark[idx]
                        x = int(point.x * frame.shape[1])
                        y = int(point.y * frame.shape[0])
                        cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)

        return frame


    def calculate_ear(self, eye):
        # EAR calculation with six points
        P2_P6 = self.distance(eye[1], eye[5])
        P3_P5 = self.distance(eye[2], eye[4])
        P1_P4 = self.distance(eye[0], eye[3])
        ear = (P2_P6 + P3_P5) / (2.0 * P1_P4)
        return ear

    def distance(self, p1, p2):
        # Calculate the Euclidean distance between two points
        x_diff = p2.x - p1.x
        y_diff = p2.y - p1.y
        return (x_diff**2 + y_diff**2)**0.5
    

app = EyeTrackingApp("MediaPipe Eye Tracking with Tkinter")


This app utilizes ai detection to track user blinks per minute in order to make break intervals for the user. However, using mediapipe's eye detection doesnt work out too well. Implement OPENCV Haar Cascades into the code, and use that to detect eye blinking instead. 
ChatGPT
To integrate OpenCV's Haar Cascades for eye detection and blinking in your application, you'll need to follow these steps:

Import the necessary OpenCV libraries: You already have cv2 imported, which is needed for OpenCV functions.

Load the Haar Cascade for eyes: OpenCV provides a pre-trained Haar Cascade classifier for eyes. You can load this classifier at the start of your application.

Modify the eye detection method: Replace the MediaPipe eye detection logic with the Haar Cascade method.

Blink Detection Logic: Use changes in the detected eye area or the absence of eyes detected as a proxy for blinking.

Here is a modified version of your EyeTrackingApp class integrating these changes:

python
Copy code
import cv2
import tkinter as tk
from PIL import Image, ImageTk
# ... (other imports)

class EyeTrackingApp:
    # ... (other methods)

    def __init__(self, window_title):
        # ... (existing initialization code)
        
        # Load the Haar Cascade for eye detection
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

        # ... (rest of the __init__ method)

    def detect_eyes(self, frame):
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        eyes = self.eye_cascade.detectMultiScale(gray, 1.1, 4)

        # Blink detection logic
        if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
            self.eye_closed = True
        elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
            self.eye_closed = False
            self.blink_count += 1
            self.update_blink_count()

        # Draw rectangles around the eyes for visualization
        for (x, y, w, h) in eyes:
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

        return frame

    # ... (rest of your class methods)

# Instantiate and run the application
app = EyeTrackingApp("OpenCV Haar Cascade Eye Tracking with Tkinter")
In this modification, I added the loading of the Haar Cascade classifier for eyes (haarcascade_eye.xml) in the __init__ method. The detect_eyes method has been reworked to use this classifier. It converts the frame to grayscale, as Haar Cascades work better on grayscale images, and then uses the detectMultiScale method to find eyes. If no eyes are detected (implying a blink), it increments the blink count.

Keep in mind that while Haar Cascades are simpler and less resource-intensive than MediaPipe, they might not be as accurate or robust in different lighting conditions or angles. Fine-tuning the parameters of detectMultiScale and testing in different scenarios will be key to improving accuracy.

User
tell me which pieces of code to DELETE, i dont want to use mediapipe anywhere. 
ChatGPT
To remove MediaPipe and solely use OpenCV Haar Cascades for eye detection in your Python application, you'll need to delete or comment out the MediaPipe-specific code and replace it with the Haar Cascade logic. Here's a guide on what to delete and modify:

Remove MediaPipe Imports: Delete the import statement for MediaPipe (import mediapipe as mp).

Remove MediaPipe FaceMesh Initialization:
Delete or comment out these lines in your __init__ method:

python
Copy code
self.mp_face_mesh = mp.solutions.face_mesh
self.face_mesh = self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)
Remove MediaPipe Logic in detect_eyes Method:
Replace the entire detect_eyes method with the Haar Cascade logic as provided in the previous response.

Remove Any Other MediaPipe References:
If there are any other lines of code that reference MediaPipe (like using self.face_mesh), remove those as well.

Add Haar Cascade Initialization:
In your __init__ method, add the line to load the Haar Cascade for eye detection:

python
Copy code
self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
Update the update Method:
If the update method contains any MediaPipe-specific logic (such as processing the frame with MediaPipe), modify it to simply pass the frame to the detect_eyes method with Haar Cascades.

Here's how your updated __init__ and detect_eyes methods might look:

python
Copy code
class EyeTrackingApp:
    def __init__(self, window_title):
        # ... (existing initialization code up to face mesh initialization)

        # Load the Haar Cascade for eye detection
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

        # ... (rest of the initialization code)

    def detect_eyes(self, frame):
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        eyes = self.eye_cascade.detectMultiScale(gray, 1.1, 4)

        # Blink detection logic
        if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
            self.eye_closed = True
        elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
            self.eye_closed = False
            self.blink_count += 1
            self.update_blink_count()

        # Draw rectangles around the eyes for visualization
        for (x, y, w, h) in eyes:
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

        return frame

    # ... (rest of your class methods)
Make sure to test the application after these changes to ensure that the eye detection and blink count functionality works as expected with the Haar Cascade method.

User
Exception in Tkinter callback
Traceback (most recent call last):
  File "C:\Users\Cyber\AppData\Local\Programs\Python\Python310\lib\tkinter\__init__.py", line 1921, in __call__
    return self.func(*args)
  File "C:\Users\Cyber\AppData\Local\Programs\Python\Python310\lib\tkinter\__init__.py", line 839, in callit
    func(*args)
  File "c:\Users\Cyber\Desktop\Python CV2\System.py", line 246, in start_updates
    self.update()
  File "c:\Users\Cyber\Desktop\Python CV2\System.py", line 258, in update
    frame = self.detect_eyes(frame)
  File "c:\Users\Cyber\Desktop\Python CV2\System.py", line 371, in detect_eyes
    results = self.face_mesh.process(image)
AttributeError: 'EyeTrackingApp' object has no attribute 'face_mesh'

Code: 
import time
import threading
import cv2
import mediapipe as mp
import tkinter as tk
from PIL import Image, ImageTk
from pynput.mouse import Listener as MouseListener
from pynput.keyboard import Listener as KeyboardListener, Key
from tkinter import ttk



class EyeTrackingApp:
    def __init__(self, window_title):
        self.root = tk.Tk()
        self.root.state('zoomed')
        self.root.title(window_title)
        self.root.configure(bg='#404040')

        self.on_break = False

        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

        # Create a style object
        style = ttk.Style()
        style.theme_use('clam')  # Use the 'clam' theme as a base for customization

        # Configure style for Button
        style.configure('Custom.TButton', 
                        background='#404040', 
                        foreground='white', 
                        bordercolor='white', 
                        borderwidth=2,
                        font=('Segoe UI', 14))

        # Configure style for Text
        text_bg = '#404040'  # Dark grey background for text
        text_fg = 'white'    # White text color

        # Main Frame
        main_frame = tk.Frame(self.root, bg='#404040')
        main_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Video Frame
        video_frame = tk.Frame(main_frame, bg='#404040')
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # Right Section Frame
        right_frame = tk.Frame(main_frame, bg='#404040')
        right_frame.pack(side=tk.RIGHT, fill=tk.Y)

        self.canvas_video = tk.Canvas(video_frame, bg='#404040')
        self.canvas_video.pack(fill="both", expand=True)

        # Time Elapsed Frame
        self.total_time_count = 0
        time_frame = tk.Frame(right_frame, bg='#404040')
        time_frame.pack(side=tk.TOP, fill=tk.X, pady=5)
        self.total_time_elapsed = tk.Label(time_frame, text="Total Time Elapsed: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_time_elapsed.pack()
        self.change_total_time_count()

        # Strictness Frame
        self.strictness = 10
        strictness_frame = tk.Frame(right_frame, bg='#404040')
        strictness_frame.pack(side=tk.TOP, fill=tk.X)
        strictness_frame.columnconfigure(0, weight=1)
        strictness_frame.columnconfigure(1, weight=1)
        self.strictness_value = tk.Label(strictness_frame, font=("Segoe UI", 20), text='Blink Strictness: ' + str(self.strictness), fg='white', bg='#404040')
        self.strictness_value.grid(row=0, columnspan=2)
        self.strictness_textbox = tk.Text(strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_strictness_button = ttk.Button(strictness_frame, text="Set Strictness", style='Custom.TButton', command=self.set_strictness)
        self.set_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.strictness_explanation = tk.Label(strictness_frame, text="Blink Strictness means the maximum blink count per minute", font=("Arial", 15), fg='white', bg='#404040')
        self.strictness_explanation.grid(row=3, columnspan=2)
        self.warning_msg = tk.Label(strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.warning_msg.grid(row=2, columnspan=2)

        # Blink Count Frame
        self.blink_count = 0
        self.EAR_THRESHOLD = 0.21
        self.eye_closed = False
        self.blink_count_frame = tk.Frame(right_frame, bg='#404040')
        self.blink_count_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_blink_count = tk.Label(self.blink_count_frame, text="Total Blink Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_blink_count.pack()

        # Break Label
        self.break_label = tk.Label(right_frame, text="Time for a break!", font=("Segoe UI", 20), fg='red', bg='#404040')
        self.break_label.pack(pady=10)
        self.break_label.pack_forget()

        # Spacer Frame
        spacer_frame = tk.Frame(right_frame, height=20, bg='#404040')
        spacer_frame.pack(side=tk.TOP, fill=tk.X)

        # Clicks and Keystrokes Frame
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        clicks_frame = tk.Frame(right_frame, bg='#404040')
        clicks_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_clicks = tk.Label(clicks_frame, text="Total Clicks: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_clicks.pack()
        keystrokes_frame = tk.Frame(right_frame, bg='#404040')
        keystrokes_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_keystrokes_label = tk.Label(keystrokes_frame, text="Total Keystroke Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_keystrokes_label.pack()

        # Total Inputs Frame
        total_inputs_frame = tk.Frame(right_frame, bg='#404040')
        total_inputs_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_inputs_label = tk.Label(total_inputs_frame, text="Total Inputs: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_inputs_label.pack()

        # Input Strictness Frame
        self.input_strictness = 50
        input_strictness_frame = tk.Frame(right_frame, bg='#404040')
        input_strictness_frame.pack(side=tk.TOP, fill=tk.X)
        input_strictness_frame.columnconfigure(0, weight=1)
        input_strictness_frame.columnconfigure(1, weight=1)
        self.input_strictness_value = tk.Label(input_strictness_frame, font=("Segoe UI", 20), text='Input Strictness: ' + str(self.input_strictness), fg='white', bg='#404040')
        self.input_strictness_value.grid(row=0, columnspan=2)
        self.input_strictness_textbox = tk.Text(input_strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.input_strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_input_strictness_button = ttk.Button(input_strictness_frame, text="Set Input Strictness", style='Custom.TButton', command=self.set_input_strictness)
        self.set_input_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.input_strictness_warning_msg = tk.Label(input_strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.input_strictness_warning_msg.grid(row=2, columnspan=2)

        # Reset Countdown Label
        self.reset_countdown_label = tk.Label(right_frame, text="Resets in 60 seconds", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.reset_countdown_label.pack(side=tk.TOP, fill=tk.X)

        # Input Listener Thread
        input_listener_thread = threading.Thread(target=self.run_input_listeners)
        input_listener_thread.daemon = True
        input_listener_thread.start()


        self.is_tracking = False
        self.video_source = 0
        self.vid = cv2.VideoCapture(self.video_source)
        self.delay = 15

        # Start video updates
        self.root.after(100, self.start_updates)
        self.handle_reset_countdown()
        self.root.mainloop()
    def set_input_strictness(self):
        value = self.input_strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 500:
                self.input_strictness = intvalue
                self.input_strictness_value.config(text="Input Strictness: " + str(intvalue))
                self.input_strictness_warning_msg.config(text="")
            else:
                self.input_strictness_warning_msg.config(text="Invalid Input! Please enter a number between 0 and 500!")
        except ValueError:
            self.input_strictness_warning_msg.config(text="Invalid Input! Please enter an integer!")

    def reset_counters(self):
        # Reset all the counters and update their respective labels
        self.blink_count = 0
        self.total_blink_count.config(text="Total Blink Count: 0")
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        self.update_click_count()
        self.update_keystroke_count()
        self.update_total_inputs_label()

    def change_total_time_count(self):
        self.total_time_elapsed.config(text=f"Total Time Elapsed: {self.total_time_count}")
        self.total_time_count += 1
        self.root.after(1000, self.change_total_time_count)

    def update_total_inputs_label(self):
        total_inputs = self.total_click_amount + self.total_keystroke_count
        self.total_inputs_label.config(text=f"Total Inputs: {total_inputs}")
        if total_inputs >= self.input_strictness and not self.on_break:
            self.initiate_break()


    def show_break_label(self):
        self.break_label.pack()

    def hide_break_label(self):
        self.break_label.pack_forget()

    def run_input_listeners(self):
        # Initialize and start the mouse listener
        mouse_listener = MouseListener(on_click=self.on_click)
        mouse_listener.start()

        # Initialize and start the keyboard listener
        keyboard_listener = KeyboardListener(on_press=self.on_press)
        keyboard_listener.start()

        # Keep the thread running
        mouse_listener.join()
        keyboard_listener.join()


    def start_eye_tracking(self):
        # Directly start tracking without checking the button state
        self.is_tracking = True
        self.canvas_video.configure(bg='black')

    def handle_reset_countdown(self, countdown=60):
        if hasattr(self, 'after_id'):  # Check if there's an existing after call
            self.root.after_cancel(self.after_id)  # Cancel the previous after call

        if self.on_break:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Break time! {countdown} seconds remaining")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.on_break = False
                self.reset_countdown_label.config(text="Resets in 60 seconds")
                self.hide_break_label()
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown
        else:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Resets in {countdown} seconds")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown


    

    def clear_video_feed(self):
        self.canvas_video.delete("all")
        self.canvas_video.configure(bg='white')
        self.canvas_video.create_text(
            self.canvas_video.winfo_width() // 2, self.canvas_video.winfo_height() // 2,
            text="Video feed stopped", font=("Arial", 20), fill="black"
        )

    def start_updates(self):
        if not self.is_tracking:
            self.start_eye_tracking()
        self.update()

    def update(self):
        ret, frame = self.vid.read()
        if ret and self.is_tracking:
            frame = cv2.flip(frame, 1)
            canvas_width = self.canvas_video.winfo_width()
            canvas_height = self.canvas_video.winfo_height()

            if canvas_width > 0 and canvas_height > 0:
                frame = self.resize_with_aspect_ratio(frame, width=canvas_width, height=canvas_height)
                if frame is not None:
                    frame = self.detect_eyes(frame)
                    self.photo = ImageTk.PhotoImage(image=Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                    self.canvas_video.create_image((canvas_width - self.photo.width()) // 2, (canvas_height - self.photo.height()) // 2, image=self.photo, anchor=tk.NW)
        self.root.after(self.delay, self.update)


    def resize_with_aspect_ratio(self, image, width=None, height=None, inter=cv2.INTER_AREA):
        (h, w) = image.shape[:2]

        # Calculate the aspect ratio of the image and the desired aspect ratio
        image_aspect = w / h
        desired_aspect = width / height if width is not None and height is not None else image_aspect

        # Calculate scaling factors for resizing the image while maintaining aspect ratio
        if image_aspect > desired_aspect:
            # Image is wider than the desired aspect ratio
            r = width / float(w)
            dim = (width, int(h * r))
        else:
            # Image is taller or equal to the desired aspect ratio
            r = height / float(h)
            dim = (int(w * r), height)

        # Check that dimensions are valid before attempting to resize
        if dim[0] > 0 and dim[1] > 0:
            resized = cv2.resize(image, dim, interpolation=inter)
            return resized
        return None


    def detect_eyes(self, frame):
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        eyes = self.eye_cascade.detectMultiScale(gray, 1.1, 4)

        # Blink detection logic
        if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
            self.eye_closed = True
        elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
            self.eye_closed = False
            self.blink_count += 1
            self.update_blink_count()

        # Draw rectangles around the eyes for visualization
        for (x, y, w, h) in eyes:
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

        return frame

    def set_strictness(self):
        value = self.strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 75:
                self.strictness = intvalue
                self.strictness_value.config(text=" Blink Strictness: " + str(intvalue))
                self.warning_msg.config(text="")
            else:
                self.warning_msg.config(text="Invalid Input! Please enter a number between 0 and 75!")
        except ValueError:
            self.warning_msg.config(text="Invalid Input! Please enter an integer!")

    def on_click(self, x, y, button, pressed):
        if pressed and not self.on_break:
            self.total_click_amount += 1
            self.update_click_count()
            self.update_total_inputs_label()

    def on_press(self, key):
        if not self.on_break:
            self.total_keystroke_count += 1
            self.update_keystroke_count()
            self.update_total_inputs_label()


    def update_click_count(self):
        self.total_clicks.config(text="Total Clicks: " + str(self.total_click_amount))
        self.update_total_inputs_label()

    def update_keystroke_count(self):
        self.total_keystrokes_label.config(text="Total Keystroke Count: " + str(self.total_keystroke_count))
    def run_mouse_listener(self):
        with MouseListener(on_click=self.on_click) as listener:
            listener.join()

    # Separate method to run the keyboard listener
    def run_keyboard_listener(self):
        with KeyboardListener(on_press=self.on_press) as listener:
            listener.join()

    """
        Eye blink engine time!
    """

    def update_blink_count(self):
        if not self.on_break:  # Only update blink count if not on a break
            self.total_blink_count.config(text=f"Total Blink Count: {self.blink_count}")

            if self.blink_count >= self.strictness:
                self.initiate_break()

    def initiate_break(self):
        # Set the state to break and update the UI accordingly
        self.on_break = True
        self.show_break_label()
        self.reset_countdown_label.config(text="Timer stopped, take a break!")
        self.handle_reset_countdown(30)  # Start a 30-second break countdown

        # Reset all counts
        self.reset_counters()

    def detect_eyes(self, frame):
        # Convert the frame to RGB for MediaPipe processing
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(image)

        # Check if any face landmarks were detected
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Hypothetical indices for the landmarks of each eye.
                # You need to replace these with the correct indices from MediaPipe
                right_eye_indices = [33, 160, 158, 133, 153, 144]  # replace with the correct indices
                left_eye_indices = [362, 385, 387, 263, 373, 380]

                # Left eye indices list
                """
                LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]
                # Right eye indices list
                RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]
                """

                # Extract the landmark coordinates for each eye
                right_eye = [face_landmarks.landmark[i] for i in right_eye_indices]
                left_eye = [face_landmarks.landmark[i] for i in left_eye_indices]

                # Calculate EAR for both eyes
                left_ear = self.calculate_ear(left_eye)
                right_ear = self.calculate_ear(right_eye)

                # Average the EAR for both eyes for better stability
                ear = (left_ear + right_ear) / 2.0

                # Blink detection logic
                if self.eye_closed and ear > self.EAR_THRESHOLD:
                    self.blink_count += 1
                    self.update_blink_count()
                    self.eye_closed = False  # Reset the eye closed flag
                elif not self.eye_closed and ear < self.EAR_THRESHOLD:
                    self.eye_closed = True  # Set the eye closed flag

                # Visualize the eye landmarks for debugging purposes
                for idx in right_eye_indices + left_eye_indices:
                    if idx < len(face_landmarks.landmark):
                        point = face_landmarks.landmark[idx]
                        x = int(point.x * frame.shape[1])
                        y = int(point.y * frame.shape[0])
                        cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)

        return frame


    def calculate_ear(self, eye):
        # EAR calculation with six points
        P2_P6 = self.distance(eye[1], eye[5])
        P3_P5 = self.distance(eye[2], eye[4])
        P1_P4 = self.distance(eye[0], eye[3])
        ear = (P2_P6 + P3_P5) / (2.0 * P1_P4)
        return ear

    def distance(self, p1, p2):
        # Calculate the Euclidean distance between two points
        x_diff = p2.x - p1.x
        y_diff = p2.y - p1.y
        return (x_diff**2 + y_diff**2)**0.5
    

app = EyeTrackingApp("MediaPipe Eye Tracking with Tkinter")
ChatGPT
The error you're encountering is because there are still references to MediaPipe (self.face_mesh) in your code. Since you've decided to use OpenCV Haar Cascades for eye detection, you need to remove all MediaPipe-related code. Here's what you need to delete or modify:

Remove MediaPipe Import:
Delete the line import mediapipe as mp.

Remove MediaPipe FaceMesh Initialization:
In your __init__ method, remove:

python
Copy code
self.mp_face_mesh = mp.solutions.face_mesh
self.face_mesh = self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)
Modify the detect_eyes Method:
The detect_eyes method at the bottom of your class seems to be still using MediaPipe logic. Replace it with the Haar Cascade logic as shown in the previous response. Make sure to completely remove the MediaPipe logic in this method.

Check for Other MediaPipe References:
Ensure there are no other references to MediaPipe throughout your code. This includes any use of self.face_mesh.

Update the update Method:
Ensure that the update method does not use any MediaPipe-related logic. It should call the updated detect_eyes method that uses Haar Cascades.

By following these steps, you should be able to eliminate the error and fully transition your application to use OpenCV's Haar Cascades for eye detection. Remember to test the application thoroughly after these changes to ensure everything functions as expected.

User
sorry, i pasted the wrong code. here is my actual updated code with the error: 
import time
import threading
import cv2
#import mediapipe as mp
import tkinter as tk
from PIL import Image, ImageTk
from pynput.mouse import Listener as MouseListener
from pynput.keyboard import Listener as KeyboardListener, Key
from tkinter import ttk



class EyeTrackingApp:
    def __init__(self, window_title):
        self.root = tk.Tk()
        self.root.state('zoomed')
        self.root.title(window_title)
        self.root.configure(bg='#404040')

        self.on_break = False

        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

        # Create a style object
        style = ttk.Style()
        style.theme_use('clam')  # Use the 'clam' theme as a base for customization

        # Configure style for Button
        style.configure('Custom.TButton', 
                        background='#404040', 
                        foreground='white', 
                        bordercolor='white', 
                        borderwidth=2,
                        font=('Segoe UI', 14))

        # Configure style for Text
        text_bg = '#404040'  # Dark grey background for text
        text_fg = 'white'    # White text color

        # Main Frame
        main_frame = tk.Frame(self.root, bg='#404040')
        main_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Video Frame
        video_frame = tk.Frame(main_frame, bg='#404040')
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # Right Section Frame
        right_frame = tk.Frame(main_frame, bg='#404040')
        right_frame.pack(side=tk.RIGHT, fill=tk.Y)

        self.canvas_video = tk.Canvas(video_frame, bg='#404040')
        self.canvas_video.pack(fill="both", expand=True)

        # Time Elapsed Frame
        self.total_time_count = 0
        time_frame = tk.Frame(right_frame, bg='#404040')
        time_frame.pack(side=tk.TOP, fill=tk.X, pady=5)
        self.total_time_elapsed = tk.Label(time_frame, text="Total Time Elapsed: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_time_elapsed.pack()
        self.change_total_time_count()

        # Strictness Frame
        self.strictness = 10
        strictness_frame = tk.Frame(right_frame, bg='#404040')
        strictness_frame.pack(side=tk.TOP, fill=tk.X)
        strictness_frame.columnconfigure(0, weight=1)
        strictness_frame.columnconfigure(1, weight=1)
        self.strictness_value = tk.Label(strictness_frame, font=("Segoe UI", 20), text='Blink Strictness: ' + str(self.strictness), fg='white', bg='#404040')
        self.strictness_value.grid(row=0, columnspan=2)
        self.strictness_textbox = tk.Text(strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_strictness_button = ttk.Button(strictness_frame, text="Set Strictness", style='Custom.TButton', command=self.set_strictness)
        self.set_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.strictness_explanation = tk.Label(strictness_frame, text="Blink Strictness means the maximum blink count per minute", font=("Arial", 15), fg='white', bg='#404040')
        self.strictness_explanation.grid(row=3, columnspan=2)
        self.warning_msg = tk.Label(strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.warning_msg.grid(row=2, columnspan=2)

        # Blink Count Frame
        self.blink_count = 0
        self.EAR_THRESHOLD = 0.21
        self.eye_closed = False
        self.blink_count_frame = tk.Frame(right_frame, bg='#404040')
        self.blink_count_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_blink_count = tk.Label(self.blink_count_frame, text="Total Blink Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_blink_count.pack()

        # Break Label
        self.break_label = tk.Label(right_frame, text="Time for a break!", font=("Segoe UI", 20), fg='red', bg='#404040')
        self.break_label.pack(pady=10)
        self.break_label.pack_forget()

        # Spacer Frame
        spacer_frame = tk.Frame(right_frame, height=20, bg='#404040')
        spacer_frame.pack(side=tk.TOP, fill=tk.X)

        # Clicks and Keystrokes Frame
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        clicks_frame = tk.Frame(right_frame, bg='#404040')
        clicks_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_clicks = tk.Label(clicks_frame, text="Total Clicks: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_clicks.pack()
        keystrokes_frame = tk.Frame(right_frame, bg='#404040')
        keystrokes_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_keystrokes_label = tk.Label(keystrokes_frame, text="Total Keystroke Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_keystrokes_label.pack()

        # Total Inputs Frame
        total_inputs_frame = tk.Frame(right_frame, bg='#404040')
        total_inputs_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_inputs_label = tk.Label(total_inputs_frame, text="Total Inputs: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_inputs_label.pack()

        # Input Strictness Frame
        self.input_strictness = 50
        input_strictness_frame = tk.Frame(right_frame, bg='#404040')
        input_strictness_frame.pack(side=tk.TOP, fill=tk.X)
        input_strictness_frame.columnconfigure(0, weight=1)
        input_strictness_frame.columnconfigure(1, weight=1)
        self.input_strictness_value = tk.Label(input_strictness_frame, font=("Segoe UI", 20), text='Input Strictness: ' + str(self.input_strictness), fg='white', bg='#404040')
        self.input_strictness_value.grid(row=0, columnspan=2)
        self.input_strictness_textbox = tk.Text(input_strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.input_strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_input_strictness_button = ttk.Button(input_strictness_frame, text="Set Input Strictness", style='Custom.TButton', command=self.set_input_strictness)
        self.set_input_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.input_strictness_warning_msg = tk.Label(input_strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.input_strictness_warning_msg.grid(row=2, columnspan=2)

        # Reset Countdown Label
        self.reset_countdown_label = tk.Label(right_frame, text="Resets in 60 seconds", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.reset_countdown_label.pack(side=tk.TOP, fill=tk.X)

        # Input Listener Thread
        input_listener_thread = threading.Thread(target=self.run_input_listeners)
        input_listener_thread.daemon = True
        input_listener_thread.start()


        self.is_tracking = False
        self.video_source = 0
        self.vid = cv2.VideoCapture(self.video_source)
        self.delay = 15

        # Start video updates
        self.root.after(100, self.start_updates)
        self.handle_reset_countdown()
        self.root.mainloop()
    def set_input_strictness(self):
        value = self.input_strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 500:
                self.input_strictness = intvalue
                self.input_strictness_value.config(text="Input Strictness: " + str(intvalue))
                self.input_strictness_warning_msg.config(text="")
            else:
                self.input_strictness_warning_msg.config(text="Invalid Input! Please enter a number between 0 and 500!")
        except ValueError:
            self.input_strictness_warning_msg.config(text="Invalid Input! Please enter an integer!")

    def reset_counters(self):
        # Reset all the counters and update their respective labels
        self.blink_count = 0
        self.total_blink_count.config(text="Total Blink Count: 0")
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        self.update_click_count()
        self.update_keystroke_count()
        self.update_total_inputs_label()

    def change_total_time_count(self):
        self.total_time_elapsed.config(text=f"Total Time Elapsed: {self.total_time_count}")
        self.total_time_count += 1
        self.root.after(1000, self.change_total_time_count)

    def update_total_inputs_label(self):
        total_inputs = self.total_click_amount + self.total_keystroke_count
        self.total_inputs_label.config(text=f"Total Inputs: {total_inputs}")
        if total_inputs >= self.input_strictness and not self.on_break:
            self.initiate_break()


    def show_break_label(self):
        self.break_label.pack()

    def hide_break_label(self):
        self.break_label.pack_forget()

    def run_input_listeners(self):
        # Initialize and start the mouse listener
        mouse_listener = MouseListener(on_click=self.on_click)
        mouse_listener.start()

        # Initialize and start the keyboard listener
        keyboard_listener = KeyboardListener(on_press=self.on_press)
        keyboard_listener.start()

        # Keep the thread running
        mouse_listener.join()
        keyboard_listener.join()


    def start_eye_tracking(self):
        # Directly start tracking without checking the button state
        self.is_tracking = True
        self.canvas_video.configure(bg='black')

    def handle_reset_countdown(self, countdown=60):
        if hasattr(self, 'after_id'):  # Check if there's an existing after call
            self.root.after_cancel(self.after_id)  # Cancel the previous after call

        if self.on_break:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Break time! {countdown} seconds remaining")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.on_break = False
                self.reset_countdown_label.config(text="Resets in 60 seconds")
                self.hide_break_label()
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown
        else:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Resets in {countdown} seconds")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown


    

    def clear_video_feed(self):
        self.canvas_video.delete("all")
        self.canvas_video.configure(bg='white')
        self.canvas_video.create_text(
            self.canvas_video.winfo_width() // 2, self.canvas_video.winfo_height() // 2,
            text="Video feed stopped", font=("Arial", 20), fill="black"
        )

    def start_updates(self):
        if not self.is_tracking:
            self.start_eye_tracking()
        self.update()

    def update(self):
        ret, frame = self.vid.read()
        if ret and self.is_tracking:
            frame = cv2.flip(frame, 1)
            canvas_width = self.canvas_video.winfo_width()
            canvas_height = self.canvas_video.winfo_height()

            if canvas_width > 0 and canvas_height > 0:
                frame = self.resize_with_aspect_ratio(frame, width=canvas_width, height=canvas_height)
                if frame is not None:
                    frame = self.detect_eyes(frame)
                    self.photo = ImageTk.PhotoImage(image=Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                    self.canvas_video.create_image((canvas_width - self.photo.width()) // 2, (canvas_height - self.photo.height()) // 2, image=self.photo, anchor=tk.NW)
        self.root.after(self.delay, self.update)


    def resize_with_aspect_ratio(self, image, width=None, height=None, inter=cv2.INTER_AREA):
        (h, w) = image.shape[:2]

        # Calculate the aspect ratio of the image and the desired aspect ratio
        image_aspect = w / h
        desired_aspect = width / height if width is not None and height is not None else image_aspect

        # Calculate scaling factors for resizing the image while maintaining aspect ratio
        if image_aspect > desired_aspect:
            # Image is wider than the desired aspect ratio
            r = width / float(w)
            dim = (width, int(h * r))
        else:
            # Image is taller or equal to the desired aspect ratio
            r = height / float(h)
            dim = (int(w * r), height)

        # Check that dimensions are valid before attempting to resize
        if dim[0] > 0 and dim[1] > 0:
            resized = cv2.resize(image, dim, interpolation=inter)
            return resized
        return None


    def detect_eyes(self, frame):
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        eyes = self.eye_cascade.detectMultiScale(gray, 1.1, 4)

        # Blink detection logic
        if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
            self.eye_closed = True
        elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
            self.eye_closed = False
            self.blink_count += 1
            self.update_blink_count()

        # Draw rectangles around the eyes for visualization
        for (x, y, w, h) in eyes:
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

        return frame

    def set_strictness(self):
        value = self.strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 75:
                self.strictness = intvalue
                self.strictness_value.config(text=" Blink Strictness: " + str(intvalue))
                self.warning_msg.config(text="")
            else:
                self.warning_msg.config(text="Invalid Input! Please enter a number between 0 and 75!")
        except ValueError:
            self.warning_msg.config(text="Invalid Input! Please enter an integer!")

    def on_click(self, x, y, button, pressed):
        if pressed and not self.on_break:
            self.total_click_amount += 1
            self.update_click_count()
            self.update_total_inputs_label()

    def on_press(self, key):
        if not self.on_break:
            self.total_keystroke_count += 1
            self.update_keystroke_count()
            self.update_total_inputs_label()


    def update_click_count(self):
        self.total_clicks.config(text="Total Clicks: " + str(self.total_click_amount))
        self.update_total_inputs_label()

    def update_keystroke_count(self):
        self.total_keystrokes_label.config(text="Total Keystroke Count: " + str(self.total_keystroke_count))
    def run_mouse_listener(self):
        with MouseListener(on_click=self.on_click) as listener:
            listener.join()

    # Separate method to run the keyboard listener
    def run_keyboard_listener(self):
        with KeyboardListener(on_press=self.on_press) as listener:
            listener.join()

    """
        Eye blink engine time!
    """

    def update_blink_count(self):
        if not self.on_break:  # Only update blink count if not on a break
            self.total_blink_count.config(text=f"Total Blink Count: {self.blink_count}")

            if self.blink_count >= self.strictness:
                self.initiate_break()

    def initiate_break(self):
        # Set the state to break and update the UI accordingly
        self.on_break = True
        self.show_break_label()
        self.reset_countdown_label.config(text="Timer stopped, take a break!")
        self.handle_reset_countdown(30)  # Start a 30-second break countdown

        # Reset all counts
        self.reset_counters()

    def detect_eyes(self, frame):
        # Convert the frame to RGB for MediaPipe processing
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(image)

        # Check if any face landmarks were detected
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Hypothetical indices for the landmarks of each eye.
                # You need to replace these with the correct indices from MediaPipe
                right_eye_indices = [33, 160, 158, 133, 153, 144]  # replace with the correct indices
                left_eye_indices = [362, 385, 387, 263, 373, 380]

                # Left eye indices list
                """
                LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]
                # Right eye indices list
                RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]
                """

                # Extract the landmark coordinates for each eye
                right_eye = [face_landmarks.landmark[i] for i in right_eye_indices]
                left_eye = [face_landmarks.landmark[i] for i in left_eye_indices]

                # Calculate EAR for both eyes
                left_ear = self.calculate_ear(left_eye)
                right_ear = self.calculate_ear(right_eye)

                # Average the EAR for both eyes for better stability
                ear = (left_ear + right_ear) / 2.0

                # Blink detection logic
                if self.eye_closed and ear > self.EAR_THRESHOLD:
                    self.blink_count += 1
                    self.update_blink_count()
                    self.eye_closed = False  # Reset the eye closed flag
                elif not self.eye_closed and ear < self.EAR_THRESHOLD:
                    self.eye_closed = True  # Set the eye closed flag

                # Visualize the eye landmarks for debugging purposes
                for idx in right_eye_indices + left_eye_indices:
                    if idx < len(face_landmarks.landmark):
                        point = face_landmarks.landmark[idx]
                        x = int(point.x * frame.shape[1])
                        y = int(point.y * frame.shape[0])
                        cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)

        return frame


    def calculate_ear(self, eye):
        # EAR calculation with six points
        P2_P6 = self.distance(eye[1], eye[5])
        P3_P5 = self.distance(eye[2], eye[4])
        P1_P4 = self.distance(eye[0], eye[3])
        ear = (P2_P6 + P3_P5) / (2.0 * P1_P4)
        return ear

    def distance(self, p1, p2):
        # Calculate the Euclidean distance between two points
        x_diff = p2.x - p1.x
        y_diff = p2.y - p1.y
        return (x_diff**2 + y_diff**2)**0.5
    

app = EyeTrackingApp("MediaPipe Eye Tracking with Tkinter")
ChatGPT
The error you're encountering is due to the presence of MediaPipe-specific code in your detect_eyes method, which is no longer valid since you've switched to using OpenCV's Haar Cascades. The self.face_mesh.process(image) line in the detect_eyes method refers to MediaPipe's FaceMesh, which you've removed from your imports and initialization.

Here are the modifications you need to make:

Remove the Old MediaPipe Eye Detection Logic:
Delete the entire section of the detect_eyes method that uses MediaPipe. This includes the part where self.face_mesh.process(image) is called and the subsequent handling of the results.

Ensure Only Haar Cascade Logic is Used in detect_eyes:
Make sure that the detect_eyes method contains only the logic for eye detection using Haar Cascades, as shown in the snippet below:

python
Copy code
def detect_eyes(self, frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    eyes = self.eye_cascade.detectMultiScale(gray, 1.1, 4)

    # Blink detection logic
    if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
        self.eye_closed = True
    elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
        self.eye_closed = False
        self.blink_count += 1
        self.update_blink_count()

    # Draw rectangles around the eyes for visualization
    for (x, y, w, h) in eyes:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

    return frame
Remove Unused Methods:
If there are any other methods or portions of code related to MediaPipe (such as calculate_ear and distance which are used for EAR calculation), and you're not using them with the Haar Cascade approach, consider removing them to clean up your code.
After making these changes, your application should no longer encounter the AttributeError and should function correctly using OpenCV's Haar Cascade for eye detection. Remember to test the updated application to ensure that it's working as expected.

User
there is an issue to this code: 
import time
import threading
import cv2
#import mediapipe as mp
import tkinter as tk
from PIL import Image, ImageTk
from pynput.mouse import Listener as MouseListener
from pynput.keyboard import Listener as KeyboardListener, Key
from tkinter import ttk



class EyeTrackingApp:
    def __init__(self, window_title):
        self.root = tk.Tk()
        self.root.state('zoomed')
        self.root.title(window_title)
        self.root.configure(bg='#404040')

        self.on_break = False

        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

        # Create a style object
        style = ttk.Style()
        style.theme_use('clam')  # Use the 'clam' theme as a base for customization

        # Configure style for Button
        style.configure('Custom.TButton', 
                        background='#404040', 
                        foreground='white', 
                        bordercolor='white', 
                        borderwidth=2,
                        font=('Segoe UI', 14))

        # Configure style for Text
        text_bg = '#404040'  # Dark grey background for text
        text_fg = 'white'    # White text color

        # Main Frame
        main_frame = tk.Frame(self.root, bg='#404040')
        main_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Video Frame
        video_frame = tk.Frame(main_frame, bg='#404040')
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # Right Section Frame
        right_frame = tk.Frame(main_frame, bg='#404040')
        right_frame.pack(side=tk.RIGHT, fill=tk.Y)

        self.canvas_video = tk.Canvas(video_frame, bg='#404040')
        self.canvas_video.pack(fill="both", expand=True)

        # Time Elapsed Frame
        self.total_time_count = 0
        time_frame = tk.Frame(right_frame, bg='#404040')
        time_frame.pack(side=tk.TOP, fill=tk.X, pady=5)
        self.total_time_elapsed = tk.Label(time_frame, text="Total Time Elapsed: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_time_elapsed.pack()
        self.change_total_time_count()

        # Strictness Frame
        self.strictness = 10
        strictness_frame = tk.Frame(right_frame, bg='#404040')
        strictness_frame.pack(side=tk.TOP, fill=tk.X)
        strictness_frame.columnconfigure(0, weight=1)
        strictness_frame.columnconfigure(1, weight=1)
        self.strictness_value = tk.Label(strictness_frame, font=("Segoe UI", 20), text='Blink Strictness: ' + str(self.strictness), fg='white', bg='#404040')
        self.strictness_value.grid(row=0, columnspan=2)
        self.strictness_textbox = tk.Text(strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_strictness_button = ttk.Button(strictness_frame, text="Set Strictness", style='Custom.TButton', command=self.set_strictness)
        self.set_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.strictness_explanation = tk.Label(strictness_frame, text="Blink Strictness means the maximum blink count per minute", font=("Arial", 15), fg='white', bg='#404040')
        self.strictness_explanation.grid(row=3, columnspan=2)
        self.warning_msg = tk.Label(strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.warning_msg.grid(row=2, columnspan=2)

        # Blink Count Frame
        self.blink_count = 0
        self.EAR_THRESHOLD = 0.21
        self.eye_closed = False
        self.blink_count_frame = tk.Frame(right_frame, bg='#404040')
        self.blink_count_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_blink_count = tk.Label(self.blink_count_frame, text="Total Blink Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_blink_count.pack()

        # Break Label
        self.break_label = tk.Label(right_frame, text="Time for a break!", font=("Segoe UI", 20), fg='red', bg='#404040')
        self.break_label.pack(pady=10)
        self.break_label.pack_forget()

        # Spacer Frame
        spacer_frame = tk.Frame(right_frame, height=20, bg='#404040')
        spacer_frame.pack(side=tk.TOP, fill=tk.X)

        # Clicks and Keystrokes Frame
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        clicks_frame = tk.Frame(right_frame, bg='#404040')
        clicks_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_clicks = tk.Label(clicks_frame, text="Total Clicks: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_clicks.pack()
        keystrokes_frame = tk.Frame(right_frame, bg='#404040')
        keystrokes_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_keystrokes_label = tk.Label(keystrokes_frame, text="Total Keystroke Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_keystrokes_label.pack()

        # Total Inputs Frame
        total_inputs_frame = tk.Frame(right_frame, bg='#404040')
        total_inputs_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_inputs_label = tk.Label(total_inputs_frame, text="Total Inputs: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_inputs_label.pack()

        # Input Strictness Frame
        self.input_strictness = 50
        input_strictness_frame = tk.Frame(right_frame, bg='#404040')
        input_strictness_frame.pack(side=tk.TOP, fill=tk.X)
        input_strictness_frame.columnconfigure(0, weight=1)
        input_strictness_frame.columnconfigure(1, weight=1)
        self.input_strictness_value = tk.Label(input_strictness_frame, font=("Segoe UI", 20), text='Input Strictness: ' + str(self.input_strictness), fg='white', bg='#404040')
        self.input_strictness_value.grid(row=0, columnspan=2)
        self.input_strictness_textbox = tk.Text(input_strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.input_strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_input_strictness_button = ttk.Button(input_strictness_frame, text="Set Input Strictness", style='Custom.TButton', command=self.set_input_strictness)
        self.set_input_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.input_strictness_warning_msg = tk.Label(input_strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.input_strictness_warning_msg.grid(row=2, columnspan=2)

        # Reset Countdown Label
        self.reset_countdown_label = tk.Label(right_frame, text="Resets in 60 seconds", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.reset_countdown_label.pack(side=tk.TOP, fill=tk.X)

        # Input Listener Thread
        input_listener_thread = threading.Thread(target=self.run_input_listeners)
        input_listener_thread.daemon = True
        input_listener_thread.start()


        self.is_tracking = False
        self.video_source = 0
        self.vid = cv2.VideoCapture(self.video_source)
        self.delay = 15

        # Start video updates
        self.root.after(100, self.start_updates)
        self.handle_reset_countdown()
        self.root.mainloop()
    def set_input_strictness(self):
        value = self.input_strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 500:
                self.input_strictness = intvalue
                self.input_strictness_value.config(text="Input Strictness: " + str(intvalue))
                self.input_strictness_warning_msg.config(text="")
            else:
                self.input_strictness_warning_msg.config(text="Invalid Input! Please enter a number between 0 and 500!")
        except ValueError:
            self.input_strictness_warning_msg.config(text="Invalid Input! Please enter an integer!")

    def reset_counters(self):
        # Reset all the counters and update their respective labels
        self.blink_count = 0
        self.total_blink_count.config(text="Total Blink Count: 0")
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        self.update_click_count()
        self.update_keystroke_count()
        self.update_total_inputs_label()

    def change_total_time_count(self):
        self.total_time_elapsed.config(text=f"Total Time Elapsed: {self.total_time_count}")
        self.total_time_count += 1
        self.root.after(1000, self.change_total_time_count)

    def update_total_inputs_label(self):
        total_inputs = self.total_click_amount + self.total_keystroke_count
        self.total_inputs_label.config(text=f"Total Inputs: {total_inputs}")
        if total_inputs >= self.input_strictness and not self.on_break:
            self.initiate_break()


    def show_break_label(self):
        self.break_label.pack()

    def hide_break_label(self):
        self.break_label.pack_forget()

    def run_input_listeners(self):
        # Initialize and start the mouse listener
        mouse_listener = MouseListener(on_click=self.on_click)
        mouse_listener.start()

        # Initialize and start the keyboard listener
        keyboard_listener = KeyboardListener(on_press=self.on_press)
        keyboard_listener.start()

        # Keep the thread running
        mouse_listener.join()
        keyboard_listener.join()


    def start_eye_tracking(self):
        # Directly start tracking without checking the button state
        self.is_tracking = True
        self.canvas_video.configure(bg='black')

    def handle_reset_countdown(self, countdown=60):
        if hasattr(self, 'after_id'):  # Check if there's an existing after call
            self.root.after_cancel(self.after_id)  # Cancel the previous after call

        if self.on_break:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Break time! {countdown} seconds remaining")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.on_break = False
                self.reset_countdown_label.config(text="Resets in 60 seconds")
                self.hide_break_label()
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown
        else:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Resets in {countdown} seconds")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown


    

    def clear_video_feed(self):
        self.canvas_video.delete("all")
        self.canvas_video.configure(bg='white')
        self.canvas_video.create_text(
            self.canvas_video.winfo_width() // 2, self.canvas_video.winfo_height() // 2,
            text="Video feed stopped", font=("Arial", 20), fill="black"
        )

    def start_updates(self):
        if not self.is_tracking:
            self.start_eye_tracking()
        self.update()

    def update(self):
        ret, frame = self.vid.read()
        if ret and self.is_tracking:
            frame = cv2.flip(frame, 1)
            canvas_width = self.canvas_video.winfo_width()
            canvas_height = self.canvas_video.winfo_height()

            if canvas_width > 0 and canvas_height > 0:
                frame = self.resize_with_aspect_ratio(frame, width=canvas_width, height=canvas_height)
                if frame is not None:
                    frame = self.detect_eyes(frame)
                    self.photo = ImageTk.PhotoImage(image=Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                    self.canvas_video.create_image((canvas_width - self.photo.width()) // 2, (canvas_height - self.photo.height()) // 2, image=self.photo, anchor=tk.NW)
        self.root.after(self.delay, self.update)


    def resize_with_aspect_ratio(self, image, width=None, height=None, inter=cv2.INTER_AREA):
        (h, w) = image.shape[:2]

        # Calculate the aspect ratio of the image and the desired aspect ratio
        image_aspect = w / h
        desired_aspect = width / height if width is not None and height is not None else image_aspect

        # Calculate scaling factors for resizing the image while maintaining aspect ratio
        if image_aspect > desired_aspect:
            # Image is wider than the desired aspect ratio
            r = width / float(w)
            dim = (width, int(h * r))
        else:
            # Image is taller or equal to the desired aspect ratio
            r = height / float(h)
            dim = (int(w * r), height)

        # Check that dimensions are valid before attempting to resize
        if dim[0] > 0 and dim[1] > 0:
            resized = cv2.resize(image, dim, interpolation=inter)
            return resized
        return None


    def detect_eyes(self, frame):
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        eyes = self.eye_cascade.detectMultiScale(gray, 1.1, 4)

        # Blink detection logic
        if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
            self.eye_closed = True
        elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
            self.eye_closed = False
            self.blink_count += 1
            self.update_blink_count()

        # Draw rectangles around the eyes for visualization
        for (x, y, w, h) in eyes:
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

        return frame

    def set_strictness(self):
        value = self.strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 75:
                self.strictness = intvalue
                self.strictness_value.config(text=" Blink Strictness: " + str(intvalue))
                self.warning_msg.config(text="")
            else:
                self.warning_msg.config(text="Invalid Input! Please enter a number between 0 and 75!")
        except ValueError:
            self.warning_msg.config(text="Invalid Input! Please enter an integer!")

    def on_click(self, x, y, button, pressed):
        if pressed and not self.on_break:
            self.total_click_amount += 1
            self.update_click_count()
            self.update_total_inputs_label()

    def on_press(self, key):
        if not self.on_break:
            self.total_keystroke_count += 1
            self.update_keystroke_count()
            self.update_total_inputs_label()


    def update_click_count(self):
        self.total_clicks.config(text="Total Clicks: " + str(self.total_click_amount))
        self.update_total_inputs_label()

    def update_keystroke_count(self):
        self.total_keystrokes_label.config(text="Total Keystroke Count: " + str(self.total_keystroke_count))
    def run_mouse_listener(self):
        with MouseListener(on_click=self.on_click) as listener:
            listener.join()

    # Separate method to run the keyboard listener
    def run_keyboard_listener(self):
        with KeyboardListener(on_press=self.on_press) as listener:
            listener.join()

    """
        Eye blink engine time!
    """

    def update_blink_count(self):
        if not self.on_break:  # Only update blink count if not on a break
            self.total_blink_count.config(text=f"Total Blink Count: {self.blink_count}")

            if self.blink_count >= self.strictness:
                self.initiate_break()

    def initiate_break(self):
        # Set the state to break and update the UI accordingly
        self.on_break = True
        self.show_break_label()
        self.reset_countdown_label.config(text="Timer stopped, take a break!")
        self.handle_reset_countdown(30)  # Start a 30-second break countdown

        # Reset all counts
        self.reset_counters()
    

app = EyeTrackingApp("MediaPipe Eye Tracking with Tkinter")
When i click or make keyboard inputs, punput isnt picking it up. I already have it installed. Can you find the error?
ChatGPT
Based on your code, it appears that the pynput listeners for mouse and keyboard inputs are being initialized and started within the run_input_listeners method. However, this method starts the listeners and then immediately joins them, which can block the main thread and prevent further execution.

To fix this issue, you should avoid using join() on the listener threads as it makes the main thread wait for these threads to complete, which they never will in this case since they are continuous listeners. Instead, you can let these listeners run in the background. Here's how you can modify the run_input_listeners method:

python
Copy code
def run_input_listeners(self):
    # Initialize the mouse listener
    mouse_listener = MouseListener(on_click=self.on_click)
    mouse_listener.start()

    # Initialize the keyboard listener
    keyboard_listener = KeyboardListener(on_press=self.on_press)
    keyboard_listener.start()

    # Do not join the threads, let them run in the background
Also, ensure that the on_click and on_press methods are correctly implemented to handle the events. For example:

python
Copy code
def on_click(self, x, y, button, pressed):
    if pressed:
        self.total_click_amount += 1
        self.update_click_count()

def on_press(self, key):
    self.total_keystroke_count += 1
    self.update_keystroke_count()
Make sure that these event handling methods (on_click and on_press) are part of your EyeTrackingApp class and are properly updating the GUI elements.

Finally, check if any exceptions are being raised and not caught. Sometimes, uncaught exceptions in background threads can cause unexpected behavior without clear error messages. If your application is still not responding to mouse and keyboard inputs, you may want to add exception handling or logging to see if the listeners are encountering any issues.

User
ok, now make the video display have a higher framerate
ChatGPT
To increase the frame rate of the video display in your application, you can reduce the delay between frame updates. This delay is controlled by the self.delay variable in your EyeTrackingApp class, which is used in the self.root.after(self.delay, self.update) call to schedule the next frame update.

However, reducing this delay too much can strain the CPU, especially if the processing done for each frame (like eye detection) is computationally intensive. It's a balancing act between achieving a higher frame rate and maintaining application performance.

Here are a few steps you can take to improve the frame rate:

Reduce the Delay: Decrease the value of self.delay. This value is in milliseconds, so a value of 15 corresponds to an attempt at about 66 FPS (1000 ms / 15 ms = ~66.67). You might try lowering it to 10 for a target of 100 FPS, but monitor the CPU usage.

Optimize Frame Processing: Ensure that the processing done on each frame is as efficient as possible. For instance, if the eye detection using Haar Cascades is taking too long, consider optimizing the parameters or resizing the frame before processing.

Avoid Unnecessary Processing: If certain processing steps are not needed for every frame, consider performing them less frequently.

Here's how you might modify the delay:

python
Copy code
class EyeTrackingApp:
    def __init__(self, window_title):
        # ... (other initialization code)
        self.delay = 10  # Reduced delay for higher frame rate
        # ... (rest of the initialization code)
And in the update method, continue using this delay as before:

python
Copy code
def update(self):
    # ... (existing code to process and display the frame)
    self.root.after(self.delay, self.update)
Remember, the actual frame rate will depend on the processing capability of your system and the complexity of the operations you're performing on each frame. Reducing the delay might not always lead to a proportionate increase in the frame rate if the system cannot process the frames quickly enough.

User
ok, now i need you to do something. Instead of drawing a big rectangle, i want you to make it a more circular shape around the eyes.
ChatGPT
To draw a more circular shape around the eyes instead of rectangles, you can modify the detect_eyes method to draw ellipses or circles based on the dimensions of the detected eye regions. The Haar Cascade detectMultiScale method returns a list of rectangles where it believes it has found eyes, with each rectangle represented by the x and y coordinates of its top-left corner, and its width and height (x, y, w, h).

Here's how you can modify the detect_eyes method to draw ellipses instead:

python
Copy code
def detect_eyes(self, frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    eyes = self.eye_cascade.detectMultiScale(gray, 1.1, 4)

    # Blink detection logic
    if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
        self.eye_closed = True
    elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
        self.eye_closed = False
        self.blink_count += 1
        self.update_blink_count()

    # Draw ellipses around the eyes for visualization
    for (x, y, w, h) in eyes:
        center = (x + w // 2, y + h // 2)  # Center of the rectangle
        axes = (w // 2, h // 2)  # Axes lengths (semi-major and semi-minor axes)
        cv2.ellipse(frame, center, axes, 0, 0, 360, (255, 0, 0), 2)

    return frame
In this modification, I've replaced the cv2.rectangle function with the cv2.ellipse function. The cv2.ellipse function draws an ellipse around each detected eye region. The center of the ellipse is calculated as the center of the rectangle, and the axes of the ellipse are half the width and height of the rectangle, making the ellipse fit the rectangular region.

This change will draw ellipses around the eyes, giving a more circular appearance than rectangles. Make sure to test this modification, as the effectiveness of the visual representation can vary depending on the accuracy of the eye detection.

User
make the elipse around the eyes, not a big circle that covers the eyelids 
ChatGPT
To draw ellipses around the eyes that fit more closely to the shape of the eyes, rather than large circles covering the eyelids, you can modify the dimensions and position of the ellipses. However, without precise eye landmarks (which Haar Cascades do not provide), it's challenging to draw perfectly fitting ellipses around the eyes.

As an approximation, you can draw smaller ellipses within the detected rectangular regions. Here's how you can adjust the detect_eyes method:

python
Copy code
def detect_eyes(self, frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    eyes = self.eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30))

    # Blink detection logic
    if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
        self.eye_closed = True
    elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
        self.eye_closed = False
        self.blink_count += 1
        self.update_blink_count()

    # Draw ellipses around the eyes for visualization
    for (x, y, w, h) in eyes:
        center = (x + w // 2, y + h // 2)
        axes = (max(5, w // 4), max(5, h // 6))  # Adjust the axes lengths to fit the eye shape better
        cv2.ellipse(frame, center, axes, 0, 0, 360, (255, 0, 0), 2)

    return frame
In this modified version:

scaleFactor and minNeighbors are used in detectMultiScale for more reliable eye detection.
minSize is set to (30, 30) to avoid detecting very small regions that are unlikely to be eyes.
The axes of the ellipses are set to a quarter of the width and a sixth of the height of the detected rectangles to better approximate the shape of the eyes. These values (w // 4 and h // 6) are chosen as an approximation and may need adjustment based on your specific requirements and the typical size of eyes in your input images.
The max function ensures that the axes lengths do not become too small, which might happen in case of very small detected rectangles.
Please note that the effectiveness of this method still depends on the accuracy of the Haar Cascade eye detector. For more precise fitting around the eyes, a more advanced technique such as using facial landmark detection (e.g., with Dlib or MediaPipe) would be necessary.

User
that didn't work. How's this: Make a limit to the amount of circles that can go on screen, like 2 for both my eyes. 
ChatGPT
Limiting the number of ellipses drawn to two (one for each eye) is a reasonable approach, especially if you're generally dealing with a frontal view of a face where both eyes are visible. You can achieve this by sorting the detected eye regions by their area (or another criterion that makes sense for your application) and then selecting the top two to draw ellipses around.

Here's how you can modify the detect_eyes method to implement this:

python
Copy code
def detect_eyes(self, frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    eyes = self.eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30))

    # Sort the detected eyes by area (w*h) and take the largest two
    eyes = sorted(eyes, key=lambda x: x[2]*x[3], reverse=True)[:2]

    # Blink detection logic
    if len(eyes) == 0 and not self.eye_closed:  # Eyes not detected (blink)
        self.eye_closed = True
    elif len(eyes) > 0 and self.eye_closed:  # Eyes detected again (end of blink)
        self.eye_closed = False
        self.blink_count += 1
        self.update_blink_count()

    # Draw ellipses around the two largest eyes for visualization
    for (x, y, w, h) in eyes:
        center = (x + w // 2, y + h // 2)
        axes = (max(5, w // 4), max(5, h // 6))
        cv2.ellipse(frame, center, axes, 0, 0, 360, (255, 0, 0), 2)

    return frame
This modified method first sorts the detected eye regions by their area and then limits the number to two. It then draws ellipses around these two regions.

Keep in mind that the Haar Cascade method for eye detection isn't always perfectly accurate, especially in varying lighting conditions or angles. It might sometimes detect other features as eyes or miss the eyes. This method will draw ellipses around the two largest regions detected as eyes, which may not always be the actual eyes depending on the situation.

For more accurate eye detection, consider using more advanced methods like Dlib's facial landmark detection or MediaPipe's Face Mesh, which offer more precise landmarks for facial features, including eyes.

User
the messy enviornment still messes with it. How's this, can you try to combine cascade WITH mediapipe? Maybe that'll work out. Make mediapipe make the area where the eye is, and make cascade to the blink detection. Mediapipe doesn't work that well with the blink, and eye cascade doesn't work well with the detecting where eyes are. 
ChatGPT
Combining OpenCV Haar Cascades with MediaPipe for eye detection and blink detection can be an effective approach. You can use MediaPipe to accurately locate the eye regions and then apply Haar Cascades within these regions for blink detection. This method leverages the strengths of both technologies: MediaPipe's precise facial landmark detection and Haar Cascade's efficiency in blink detection.

Here's how you can modify your application to use both:

Reintroduce MediaPipe: Import MediaPipe and initialize it in your __init__ method. You'll use MediaPipe to get the facial landmarks and then focus the Haar Cascade eye detection within these regions.

Detect Eyes with MediaPipe: Use MediaPipe to detect facial landmarks and identify the regions around the eyes.

Apply Haar Cascade for Blink Detection: Use the eye regions obtained from MediaPipe as the regions of interest (ROI) for Haar Cascade to detect blinks.

Here is a modified version of your application with these changes:

First, ensure MediaPipe is installed:

bash
Copy code
pip install mediapipe
Then, modify your code:

python
Copy code
import cv2
import mediapipe as mp
# ... (other imports)

class EyeTrackingApp:
    def __init__(self, window_title):
        # ... (other initialization code)
        
        # Initialize MediaPipe Face Mesh
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)

        # Initialize Haar Cascade for eye blink detection
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        
        # ... (rest of the initialization code)

    def detect_eyes(self, frame):
        # First, use MediaPipe to find facial landmarks
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_frame)

        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Get the eye regions using facial landmarks
                # Assume landmarks 33, 133, 362, and 263 are the tips of the eyes
                for idx in [33, 133, 362, 263]:
                    if idx < len(face_landmarks.landmark):
                        point = face_landmarks.landmark[idx]
                        x = int(point.x * frame.shape[1])
                        y = int(point.y * frame.shape[0])
                        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  # Just for visualization

                        # Define ROI for Haar Cascade (simple rectangular around the eye)
                        # You might want to adjust the size of the ROI as per your requirements
                        roi_x1 = max(x - 25, 0)
                        roi_y1 = max(y - 25, 0)
                        roi_x2 = min(x + 25, frame.shape[1])
                        roi_y2 = min(y + 25, frame.shape[0])
                        roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]

                        # Apply Haar Cascade for each eye region (ROI)
                        eyes = self.eye_cascade.detectMultiScale(roi)
                        for (ex, ey, ew, eh) in eyes:
                            cv2.rectangle(roi, (ex, ey), (ex+ew, ey+eh), (255, 0, 0), 2)

        # ... (rest of the detect_eyes method)
        
# ... (rest of your EyeTrackingApp class)
This code uses MediaPipe to find the eye regions and then applies Haar Cascade within these regions to detect blinks. The ROIs for the eyes are determined using specific facial landmarks. You might need to adjust the size and position of these ROIs based on the landmarks to better fit the eye areas.

This approach provides a more targeted area for Haar Cascade to work on, potentially improving blink detection accuracy while using the precise facial landmark detection capabilities of MediaPipe.

User
give me the full chunk of the code of camera configuration with opencv, mediapipe and cascade. 
ChatGPT
Certainly! To create a Python application that uses both OpenCV and MediaPipe for camera configuration and eye blink detection, here's a comprehensive example. This code integrates OpenCV for video capture, MediaPipe for accurate facial landmark detection, and Haar Cascade for eye blink detection within the regions defined by MediaPipe.

Before you run the code, ensure you have the necessary packages installed:

bash
Copy code
pip install opencv-python mediapipe
Here's the full code:

python
Copy code
import cv2
import mediapipe as mp
import tkinter as tk
from PIL import Image, ImageTk
from threading import Thread

class EyeTrackingApp:
    def __init__(self, window_title):
        self.root = tk.Tk()
        self.root.title(window_title)
        self.root.configure(bg='#404040')

        # MediaPipe setup
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)

        # Haar Cascade setup
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

        # Video capture setup
        self.video_source = 0
        self.vid = cv2.VideoCapture(self.video_source)
        self.width = self.vid.get(cv2.CAP_PROP_FRAME_WIDTH)
        self.height = self.vid.get(cv2.CAP_PROP_FRAME_HEIGHT)

        # GUI setup
        self.canvas = tk.Canvas(self.root, width=self.width, height=self.height)
        self.canvas.pack()

        # Start video stream in a thread
        self.thread = Thread(target=self.video_stream)
        self.thread.daemon = 1
        self.thread.start()

    def video_stream(self):
        while True:
            ret, frame = self.vid.read()
            if ret:
                frame = self.detect_eyes(frame)
                self.photo = ImageTk.PhotoImage(image=Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                self.canvas.create_image(0, 0, image=self.photo, anchor=tk.NW)
            self.root.update()

    def detect_eyes(self, frame):
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_frame)

        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                for idx in [33, 133, 362, 263]:  # Landmarks for eye corners
                    if idx < len(face_landmarks.landmark):
                        point = face_landmarks.landmark[idx]
                        x, y = int(point.x * frame.shape[1]), int(point.y * frame.shape[0])
                        roi_x1, roi_y1 = max(x - 25, 0), max(y - 25, 0)
                        roi_x2, roi_y2 = min(x + 25, frame.shape[1]), min(y + 25, frame.shape[0])
                        roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]

                        eyes = self.eye_cascade.detectMultiScale(roi)
                        for (ex, ey, ew, eh) in eyes:
                            cv2.rectangle(roi, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

        return frame

    def __del__(self):
        self.vid.release()

app = EyeTrackingApp("Eye Tracking with OpenCV and MediaPipe")
app.root.mainloop()
This code creates a Tkinter GUI to display the video feed. MediaPipe's Face Mesh is used to find facial landmarks, and based on these landmarks, regions of interest (ROIs) are defined around the eyes. Haar Cascade is then applied to these ROIs to detect blinks. The video stream runs in a separate thread to ensure the GUI remains responsive.

Note:

The landmarks 33, 133, 362, and 263 are used as reference points for the eyes. These may need to
User
send me the full detect eyes method



ChatGPT
Certainly! Here's the full detect_eyes method from the previous example. This method uses MediaPipe for facial landmark detection and applies the Haar Cascade for blink detection within the eye regions defined by these landmarks:

python
Copy code
def detect_eyes(self, frame):
    # Convert the frame to RGB for MediaPipe processing
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = self.face_mesh.process(rgb_frame)

    # Check if any face landmarks were detected
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            # Define regions of interest (ROIs) around the eyes based on specific landmarks
            for idx in [33, 133, 362, 263]:  # Landmarks for eye corners
                if idx < len(face_landmarks.landmark):
                    point = face_landmarks.landmark[idx]
                    x, y = int(point.x * frame.shape[1]), int(point.y * frame.shape[0])
                    # Adjust the size of the ROI as needed
                    roi_x1, roi_y1 = max(x - 25, 0), max(y - 25, 0)
                    roi_x2, roi_y2 = min(x + 25, frame.shape[1]), min(y + 25, frame.shape[0])
                    roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]

                    # Apply Haar Cascade to detect eyes within the ROI
                    eyes = self.eye_cascade.detectMultiScale(roi)
                    for (ex, ey, ew, eh) in eyes:
                        # Draw rectangles around detected eyes for visualization
                        cv2.rectangle(roi, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

    return frame
In this method:

The frame is first converted to RGB as required by MediaPipe.
MediaPipe's Face Mesh is used to process the frame and detect facial landmarks.
For each detected face, the method identifies regions of interest (ROIs) around the eyes using specific landmarks. Here, landmarks 33, 133, 362, and 263 are used as reference points for the eye corners.
The Haar Cascade detectMultiScale function is applied to each ROI to detect eyes.
Detected eyes within each ROI are marked with rectangles for visualization purposes.
The dimensions of the ROIs (the 25 pixel offsets in roi_x1, roi_y1, roi_x2, roi_y2) may need adjustment based on the specific requirements of your application and the size of the eyes in the input images.

User
i have the eye indices, give me a method to put it in the detection logic. Two lists of indices for right and left eyes would work. 
ChatGPT
If you have specific indices for the landmarks of the right and left eyes from MediaPipe, you can use these indices to define more precise regions of interest (ROIs) for each eye. This will allow for more accurate detection and visualization.

Assuming you have two lists, RIGHT_EYE_INDICES and LEFT_EYE_INDICES, which contain the indices of the landmarks for the right and left eyes respectively, you can modify the detect_eyes method to use these indices.

Here's how you can implement this:

python
Copy code
RIGHT_EYE_INDICES = [33, 133, 160, 144, 145, 153, 154, 155]  # Replace with your right eye indices
LEFT_EYE_INDICES = [362, 263, 387, 373, 374, 380, 381, 382]  # Replace with your left eye indices

def detect_eyes(self, frame):
    # Convert the frame to RGB for MediaPipe processing
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = self.face_mesh.process(rgb_frame)

    # Check if any face landmarks were detected
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            # Process each eye
            for eye_indices in [RIGHT_EYE_INDICES, LEFT_EYE_INDICES]:
                # Calculate the bounding box for the eye
                eye_coords = [(int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])) 
                              for idx in eye_indices 
                              for landmark in [face_landmarks.landmark[idx]]]

                if eye_coords:
                    # Calculate the bounding box around the eye landmarks
                    x1, y1 = min([coord[0] for coord in eye_coords]), min([coord[1] for coord in eye_coords])
                    x2, y2 = max([coord[0] for coord in eye_coords]), max([coord[1] for coord in eye_coords])
                    eye_roi = frame[y1:y2, x1:x2]

                    # Apply Haar Cascade to detect blinks within the ROI
                    eyes = self.eye_cascade.detectMultiScale(eye_roi)
                    for (ex, ey, ew, eh) in eyes:
                        # Draw rectangles around detected eyes for visualization
                        cv2.rectangle(eye_roi, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

    return frame
In this method:

The landmarks for each eye are used to calculate the bounding box (ROI) surrounding the eye.
The Haar Cascade detectMultiScale method is then applied within each eye's ROI to detect blinks.
Detected blinks are visualized with rectangles.
This approach offers more precise control over the eye regions being analyzed and should improve the accuracy of your blink detection. Remember to adjust the indices in RIGHT_EYE_INDICES and LEFT_EYE_INDICES to match the specific landmarks for the eyes as defined in your application.

User
my code: 
import time
import threading
import cv2
import mediapipe as mp
import tkinter as tk
from PIL import Image, ImageTk
from pynput.mouse import Listener as MouseListener
from pynput.keyboard import Listener as KeyboardListener, Key
from tkinter import ttk



class EyeTrackingApp:
    def __init__(self, window_title):
        self.root = tk.Tk()
        self.root.state('zoomed')
        self.root.title(window_title)
        self.root.configure(bg='#404040')

        self.on_break = False


        # Create a style object
        style = ttk.Style()
        style.theme_use('clam')  # Use the 'clam' theme as a base for customization

        # Configure style for Button
        style.configure('Custom.TButton', 
                        background='#404040', 
                        foreground='white', 
                        bordercolor='white', 
                        borderwidth=2,
                        font=('Segoe UI', 14))

        # Configure style for Text
        text_bg = '#404040'  # Dark grey background for text
        text_fg = 'white'    # White text color

        # Main Frame
        main_frame = tk.Frame(self.root, bg='#404040')
        main_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Video Frame
        video_frame = tk.Frame(main_frame, bg='#404040')
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # Right Section Frame
        right_frame = tk.Frame(main_frame, bg='#404040')
        right_frame.pack(side=tk.RIGHT, fill=tk.Y)

        self.canvas_video = tk.Canvas(video_frame, bg='#404040')
        self.canvas_video.pack(fill="both", expand=True)

        # Time Elapsed Frame
        self.total_time_count = 0
        time_frame = tk.Frame(right_frame, bg='#404040')
        time_frame.pack(side=tk.TOP, fill=tk.X, pady=5)
        self.total_time_elapsed = tk.Label(time_frame, text="Total Time Elapsed: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_time_elapsed.pack()
        self.change_total_time_count()

        # Strictness Frame
        self.strictness = 10
        strictness_frame = tk.Frame(right_frame, bg='#404040')
        strictness_frame.pack(side=tk.TOP, fill=tk.X)
        strictness_frame.columnconfigure(0, weight=1)
        strictness_frame.columnconfigure(1, weight=1)
        self.strictness_value = tk.Label(strictness_frame, font=("Segoe UI", 20), text='Blink Strictness: ' + str(self.strictness), fg='white', bg='#404040')
        self.strictness_value.grid(row=0, columnspan=2)
        self.strictness_textbox = tk.Text(strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_strictness_button = ttk.Button(strictness_frame, text="Set Strictness", style='Custom.TButton', command=self.set_strictness)
        self.set_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.strictness_explanation = tk.Label(strictness_frame, text="Blink Strictness means the maximum blink count per minute", font=("Arial", 15), fg='white', bg='#404040')
        self.strictness_explanation.grid(row=3, columnspan=2)
        self.warning_msg = tk.Label(strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.warning_msg.grid(row=2, columnspan=2)

        # Blink Count Frame
        self.blink_count = 0
        self.EAR_THRESHOLD = 0.21
        self.eye_closed = False
        self.blink_count_frame = tk.Frame(right_frame, bg='#404040')
        self.blink_count_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_blink_count = tk.Label(self.blink_count_frame, text="Total Blink Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_blink_count.pack()

        # Break Label
        self.break_label = tk.Label(right_frame, text="Time for a break!", font=("Segoe UI", 20), fg='red', bg='#404040')
        self.break_label.pack(pady=10)
        self.break_label.pack_forget()

        # Spacer Frame
        spacer_frame = tk.Frame(right_frame, height=20, bg='#404040')
        spacer_frame.pack(side=tk.TOP, fill=tk.X)

        # Clicks and Keystrokes Frame
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        clicks_frame = tk.Frame(right_frame, bg='#404040')
        clicks_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_clicks = tk.Label(clicks_frame, text="Total Clicks: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_clicks.pack()
        keystrokes_frame = tk.Frame(right_frame, bg='#404040')
        keystrokes_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_keystrokes_label = tk.Label(keystrokes_frame, text="Total Keystroke Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_keystrokes_label.pack()

        # Total Inputs Frame
        total_inputs_frame = tk.Frame(right_frame, bg='#404040')
        total_inputs_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_inputs_label = tk.Label(total_inputs_frame, text="Total Inputs: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_inputs_label.pack()

        # Input Strictness Frame
        self.input_strictness = 50
        input_strictness_frame = tk.Frame(right_frame, bg='#404040')
        input_strictness_frame.pack(side=tk.TOP, fill=tk.X)
        input_strictness_frame.columnconfigure(0, weight=1)
        input_strictness_frame.columnconfigure(1, weight=1)
        self.input_strictness_value = tk.Label(input_strictness_frame, font=("Segoe UI", 20), text='Input Strictness: ' + str(self.input_strictness), fg='white', bg='#404040')
        self.input_strictness_value.grid(row=0, columnspan=2)
        self.input_strictness_textbox = tk.Text(input_strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.input_strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_input_strictness_button = ttk.Button(input_strictness_frame, text="Set Input Strictness", style='Custom.TButton', command=self.set_input_strictness)
        self.set_input_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.input_strictness_warning_msg = tk.Label(input_strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.input_strictness_warning_msg.grid(row=2, columnspan=2)

        # Reset Countdown Label
        self.reset_countdown_label = tk.Label(right_frame, text="Resets in 60 seconds", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.reset_countdown_label.pack(side=tk.TOP, fill=tk.X)

        # Input Listener Thread
        input_listener_thread = threading.Thread(target=self.run_input_listeners)
        input_listener_thread.daemon = True
        input_listener_thread.start()

        self.is_tracking = False
        self.video_source = 0
        self.vid = cv2.VideoCapture(self.video_source)
        self.delay = 15

        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)

        # Initialize Haar Cascade for eye blink detection
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        


        # Start video updates
        self.root.after(100, self.start_updates)
        self.handle_reset_countdown()
        self.root.mainloop()
    def set_input_strictness(self):
        value = self.input_strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 500:
                self.input_strictness = intvalue
                self.input_strictness_value.config(text="Input Strictness: " + str(intvalue))
                self.input_strictness_warning_msg.config(text="")
            else:
                self.input_strictness_warning_msg.config(text="Invalid Input! Please enter a number between 0 and 500!")
        except ValueError:
            self.input_strictness_warning_msg.config(text="Invalid Input! Please enter an integer!")

    def reset_counters(self):
        # Reset all the counters and update their respective labels
        self.blink_count = 0
        self.total_blink_count.config(text="Total Blink Count: 0")
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        self.update_click_count()
        self.update_keystroke_count()
        self.update_total_inputs_label()

    def change_total_time_count(self):
        self.total_time_elapsed.config(text=f"Total Time Elapsed: {self.total_time_count}")
        self.total_time_count += 1
        self.root.after(1000, self.change_total_time_count)

    def update_total_inputs_label(self):
        total_inputs = self.total_click_amount + self.total_keystroke_count
        self.total_inputs_label.config(text=f"Total Inputs: {total_inputs}")
        if total_inputs >= self.input_strictness and not self.on_break:
            self.initiate_break()


    def show_break_label(self):
        self.break_label.pack()

    def hide_break_label(self):
        self.break_label.pack_forget()

    def run_input_listeners(self):
        # Initialize the mouse listener
        mouse_listener = MouseListener(on_click=self.on_click)
        mouse_listener.start()

        # Initialize the keyboard listener
        keyboard_listener = KeyboardListener(on_press=self.on_press)
        keyboard_listener.start()


    def start_eye_tracking(self):
        # Directly start tracking without checking the button state
        self.is_tracking = True
        self.canvas_video.configure(bg='black')

    def handle_reset_countdown(self, countdown=60):
        if hasattr(self, 'after_id'):  # Check if there's an existing after call
            self.root.after_cancel(self.after_id)  # Cancel the previous after call

        if self.on_break:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Break time! {countdown} seconds remaining")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.on_break = False
                self.reset_countdown_label.config(text="Resets in 60 seconds")
                self.hide_break_label()
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown
        else:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Resets in {countdown} seconds")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown


    

    def clear_video_feed(self):
        self.canvas_video.delete("all")
        self.canvas_video.configure(bg='white')
        self.canvas_video.create_text(
            self.canvas_video.winfo_width() // 2, self.canvas_video.winfo_height() // 2,
            text="Video feed stopped", font=("Arial", 20), fill="black"
        )

    def start_updates(self):
        if not self.is_tracking:
            self.start_eye_tracking()
        self.update()

    def update(self):
        ret, frame = self.vid.read()
        if ret and self.is_tracking:
            frame = cv2.flip(frame, 1)
            canvas_width = self.canvas_video.winfo_width()
            canvas_height = self.canvas_video.winfo_height()

            if canvas_width > 0 and canvas_height > 0:
                frame = self.resize_with_aspect_ratio(frame, width=canvas_width, height=canvas_height)
                if frame is not None:
                    frame = self.detect_eyes(frame)
                    self.photo = ImageTk.PhotoImage(image=Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                    self.canvas_video.create_image((canvas_width - self.photo.width()) // 2, (canvas_height - self.photo.height()) // 2, image=self.photo, anchor=tk.NW)
        self.root.after(self.delay, self.update)


    def resize_with_aspect_ratio(self, image, width=None, height=None, inter=cv2.INTER_AREA):
        (h, w) = image.shape[:2]

        # Calculate the aspect ratio of the image and the desired aspect ratio
        image_aspect = w / h
        desired_aspect = width / height if width is not None and height is not None else image_aspect

        # Calculate scaling factors for resizing the image while maintaining aspect ratio
        if image_aspect > desired_aspect:
            # Image is wider than the desired aspect ratio
            r = width / float(w)
            dim = (width, int(h * r))
        else:
            # Image is taller or equal to the desired aspect ratio
            r = height / float(h)
            dim = (int(w * r), height)

        # Check that dimensions are valid before attempting to resize
        if dim[0] > 0 and dim[1] > 0:
            resized = cv2.resize(image, dim, interpolation=inter)
            return resized
        return None




    def detect_eyes(self, frame):
        LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]
        RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]
        # Convert the frame to RGB for MediaPipe processing
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_frame)

        # Check if any face landmarks were detected
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Process each eye
                for eye_indices in [RIGHT_EYE_INDICES, LEFT_EYE_INDICES]:
                    # Calculate the bounding box for the eye
                    eye_coords = [(int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])) 
                                for idx in eye_indices 
                                for landmark in [face_landmarks.landmark[idx]]]
                    if eye_coords:
                        # Calculate the bounding box around the eye landmarks
                        x1, y1 = min([coord[0] for coord in eye_coords]), min([coord[1] for coord in eye_coords])
                        x2, y2 = max([coord[0] for coord in eye_coords]), max([coord[1] for coord in eye_coords])
                        eye_roi = frame[y1:y2, x1:x2]

                        # Apply Haar Cascade to detect blinks within the ROI
                        eyes = self.eye_cascade.detectMultiScale(eye_roi)
                        for (ex, ey, ew, eh) in eyes:
                            # Draw rectangles around detected eyes for visualization
                            cv2.rectangle(eye_roi, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

        return frame




    def set_strictness(self):
        value = self.strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 75:
                self.strictness = intvalue
                self.strictness_value.config(text=" Blink Strictness: " + str(intvalue))
                self.warning_msg.config(text="")
            else:
                self.warning_msg.config(text="Invalid Input! Please enter a number between 0 and 75!")
        except ValueError:
            self.warning_msg.config(text="Invalid Input! Please enter an integer!")

   
    def on_click(self, x, y, button, pressed):
        if pressed:
            self.total_click_amount += 1
            self.update_click_count()
            self.update_total_inputs_label()

    def on_press(self, key):
        self.total_keystroke_count += 1
        self.update_keystroke_count()
        self.update_total_inputs_label()


    def update_click_count(self):
        self.total_clicks.config(text="Total Clicks: " + str(self.total_click_amount))
        self.update_total_inputs_label()

    def update_keystroke_count(self):
        self.total_keystrokes_label.config(text="Total Keystroke Count: " + str(self.total_keystroke_count))
    def run_mouse_listener(self):
        with MouseListener(on_click=self.on_click) as listener:
            listener.join()

    # Separate method to run the keyboard listener
    def run_keyboard_listener(self):
        with KeyboardListener(on_press=self.on_press) as listener:
            listener.join()

    """
        Eye blink engine time!
    """

    def update_blink_count(self):
        if not self.on_break:  # Only update blink count if not on a break
            self.total_blink_count.config(text=f"Total Blink Count: {self.blink_count}")

            if self.blink_count >= self.strictness:
                self.initiate_break()

    def initiate_break(self):
        # Set the state to break and update the UI accordingly
        self.on_break = True
        self.show_break_label()
        self.reset_countdown_label.config(text="Timer stopped, take a break!")
        self.handle_reset_countdown(30)  # Start a 30-second break countdown

        # Reset all counts
        self.reset_counters()
    

app = EyeTrackingApp("MediaPipe Eye Tracking with Tkinter")

Error i got: 
PS C:\Users\chaot\Desktop\Jimmy's Cabin\Python CV2> & C:/Users/chaot/AppData/Local/Programs/Python/Python310/python.exe "c:/Users/chaot/Desktop/Jimmy's Cabin/Python CV2/System.py"
Traceback (most recent call last):
  File "c:\Users\chaot\Desktop\Jimmy's Cabin\Python CV2\System.py", line 4, in <module>
    import mediapipe as mp
  File "C:\Users\chaot\AppData\Local\Programs\Python\Python310\lib\site-packages\mediapipe\__init__.py", line 15, in <module>
    from mediapipe.python import *
  File "C:\Users\chaot\AppData\Local\Programs\Python\Python310\lib\site-packages\mediapipe\python\__init__.py", line 17, in <module>
    from mediapipe.python._framework_bindings import resource_util
ImportError: DLL load failed while importing _framework_bindings: The specified module could not be found.
ChatGPT
The error you're encountering, ImportError: DLL load failed while importing _framework_bindings: The specified module could not be found, suggests there's an issue with the MediaPipe installation, specifically with its native dependencies.

Here are a few steps to resolve this issue:

Ensure All Dependencies Are Installed: MediaPipe requires some additional dependencies. Make sure you have the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019 installed. These can be found on the Microsoft website.

Reinstall MediaPipe: Sometimes, reinstalling the package can resolve these issues. You can do this via pip:

bash
Copy code
pip uninstall mediapipe
pip install mediapipe
Check Your Python Version: Ensure you are using a Python version that is compatible with MediaPipe. As of my last update, MediaPipe was compatible with Python 3.7, 3.8, and 3.9. If you are using Python 3.10, you might encounter compatibility issues.

Check System Architecture: Ensure that your Python architecture (32-bit vs 64-bit) matches your system's architecture. MediaPipe is usually more compatible with a 64-bit Python installation on a 64-bit system.

Update Your System PATH: Sometimes, the necessary DLLs are present on your system, but Python cannot find them because they are not in the PATH. Adding the folder containing the DLLs to your system PATH might resolve the issue.

Try a Different Environment: If you're still facing issues, try setting up a new Python environment using virtualenv or conda, and then install MediaPipe within that environment.

If the problem persists after trying these solutions, it might be helpful to look for any open issues on the MediaPipe GitHub repository or consider posting your issue there. Compatibility issues can sometimes be specific to the system configuration, and the developers or community may have more direct insights.

User
there is a problem: I have installed visual c++, and my code is here: 
import time
import threading
import cv2
import mediapipe as mp
import tkinter as tk
from PIL import Image, ImageTk
from pynput.mouse import Listener as MouseListener
from pynput.keyboard import Listener as KeyboardListener, Key
from tkinter import ttk



class EyeTrackingApp:
    def __init__(self, window_title):
        self.root = tk.Tk()
        self.root.state('zoomed')
        self.root.title(window_title)
        self.root.configure(bg='#404040')

        self.on_break = False


        # Create a style object
        style = ttk.Style()
        style.theme_use('clam')  # Use the 'clam' theme as a base for customization

        # Configure style for Button
        style.configure('Custom.TButton', 
                        background='#404040', 
                        foreground='white', 
                        bordercolor='white', 
                        borderwidth=2,
                        font=('Segoe UI', 14))

        # Configure style for Text
        text_bg = '#404040'  # Dark grey background for text
        text_fg = 'white'    # White text color

        # Main Frame
        main_frame = tk.Frame(self.root, bg='#404040')
        main_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Video Frame
        video_frame = tk.Frame(main_frame, bg='#404040')
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # Right Section Frame
        right_frame = tk.Frame(main_frame, bg='#404040')
        right_frame.pack(side=tk.RIGHT, fill=tk.Y)

        self.canvas_video = tk.Canvas(video_frame, bg='#404040')
        self.canvas_video.pack(fill="both", expand=True)

        # Time Elapsed Frame
        self.total_time_count = 0
        time_frame = tk.Frame(right_frame, bg='#404040')
        time_frame.pack(side=tk.TOP, fill=tk.X, pady=5)
        self.total_time_elapsed = tk.Label(time_frame, text="Total Time Elapsed: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_time_elapsed.pack()
        self.change_total_time_count()

        # Strictness Frame
        self.strictness = 10
        strictness_frame = tk.Frame(right_frame, bg='#404040')
        strictness_frame.pack(side=tk.TOP, fill=tk.X)
        strictness_frame.columnconfigure(0, weight=1)
        strictness_frame.columnconfigure(1, weight=1)
        self.strictness_value = tk.Label(strictness_frame, font=("Segoe UI", 20), text='Blink Strictness: ' + str(self.strictness), fg='white', bg='#404040')
        self.strictness_value.grid(row=0, columnspan=2)
        self.strictness_textbox = tk.Text(strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_strictness_button = ttk.Button(strictness_frame, text="Set Strictness", style='Custom.TButton', command=self.set_strictness)
        self.set_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.strictness_explanation = tk.Label(strictness_frame, text="Blink Strictness means the maximum blink count per minute", font=("Arial", 15), fg='white', bg='#404040')
        self.strictness_explanation.grid(row=3, columnspan=2)
        self.warning_msg = tk.Label(strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.warning_msg.grid(row=2, columnspan=2)

        # Blink Count Frame
        self.blink_count = 0
        self.EAR_THRESHOLD = 0.21
        self.eye_closed = False
        self.blink_count_frame = tk.Frame(right_frame, bg='#404040')
        self.blink_count_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_blink_count = tk.Label(self.blink_count_frame, text="Total Blink Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_blink_count.pack()

        # Break Label
        self.break_label = tk.Label(right_frame, text="Time for a break!", font=("Segoe UI", 20), fg='red', bg='#404040')
        self.break_label.pack(pady=10)
        self.break_label.pack_forget()

        # Spacer Frame
        spacer_frame = tk.Frame(right_frame, height=20, bg='#404040')
        spacer_frame.pack(side=tk.TOP, fill=tk.X)

        # Clicks and Keystrokes Frame
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        clicks_frame = tk.Frame(right_frame, bg='#404040')
        clicks_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_clicks = tk.Label(clicks_frame, text="Total Clicks: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_clicks.pack()
        keystrokes_frame = tk.Frame(right_frame, bg='#404040')
        keystrokes_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_keystrokes_label = tk.Label(keystrokes_frame, text="Total Keystroke Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_keystrokes_label.pack()

        # Total Inputs Frame
        total_inputs_frame = tk.Frame(right_frame, bg='#404040')
        total_inputs_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_inputs_label = tk.Label(total_inputs_frame, text="Total Inputs: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_inputs_label.pack()

        # Input Strictness Frame
        self.input_strictness = 50
        input_strictness_frame = tk.Frame(right_frame, bg='#404040')
        input_strictness_frame.pack(side=tk.TOP, fill=tk.X)
        input_strictness_frame.columnconfigure(0, weight=1)
        input_strictness_frame.columnconfigure(1, weight=1)
        self.input_strictness_value = tk.Label(input_strictness_frame, font=("Segoe UI", 20), text='Input Strictness: ' + str(self.input_strictness), fg='white', bg='#404040')
        self.input_strictness_value.grid(row=0, columnspan=2)
        self.input_strictness_textbox = tk.Text(input_strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.input_strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_input_strictness_button = ttk.Button(input_strictness_frame, text="Set Input Strictness", style='Custom.TButton', command=self.set_input_strictness)
        self.set_input_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.input_strictness_warning_msg = tk.Label(input_strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.input_strictness_warning_msg.grid(row=2, columnspan=2)

        # Reset Countdown Label
        self.reset_countdown_label = tk.Label(right_frame, text="Resets in 60 seconds", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.reset_countdown_label.pack(side=tk.TOP, fill=tk.X)

        # Input Listener Thread
        input_listener_thread = threading.Thread(target=self.run_input_listeners)
        input_listener_thread.daemon = True
        input_listener_thread.start()

        self.is_tracking = False
        self.video_source = 0
        self.vid = cv2.VideoCapture(self.video_source)
        self.delay = 15

        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)

        # Initialize Haar Cascade for eye blink detection
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        


        # Start video updates
        self.root.after(100, self.start_updates)
        self.handle_reset_countdown()
        self.root.mainloop()
    def set_input_strictness(self):
        value = self.input_strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 500:
                self.input_strictness = intvalue
                self.input_strictness_value.config(text="Input Strictness: " + str(intvalue))
                self.input_strictness_warning_msg.config(text="")
            else:
                self.input_strictness_warning_msg.config(text="Invalid Input! Please enter a number between 0 and 500!")
        except ValueError:
            self.input_strictness_warning_msg.config(text="Invalid Input! Please enter an integer!")

    def reset_counters(self):
        # Reset all the counters and update their respective labels
        self.blink_count = 0
        self.total_blink_count.config(text="Total Blink Count: 0")
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        self.update_click_count()
        self.update_keystroke_count()
        self.update_total_inputs_label()

    def change_total_time_count(self):
        self.total_time_elapsed.config(text=f"Total Time Elapsed: {self.total_time_count}")
        self.total_time_count += 1
        self.root.after(1000, self.change_total_time_count)

    def update_total_inputs_label(self):
        total_inputs = self.total_click_amount + self.total_keystroke_count
        self.total_inputs_label.config(text=f"Total Inputs: {total_inputs}")
        if total_inputs >= self.input_strictness and not self.on_break:
            self.initiate_break()


    def show_break_label(self):
        self.break_label.pack()

    def hide_break_label(self):
        self.break_label.pack_forget()

    def run_input_listeners(self):
        # Initialize the mouse listener
        mouse_listener = MouseListener(on_click=self.on_click)
        mouse_listener.start()

        # Initialize the keyboard listener
        keyboard_listener = KeyboardListener(on_press=self.on_press)
        keyboard_listener.start()


    def start_eye_tracking(self):
        # Directly start tracking without checking the button state
        self.is_tracking = True
        self.canvas_video.configure(bg='black')

    def handle_reset_countdown(self, countdown=60):
        if hasattr(self, 'after_id'):  # Check if there's an existing after call
            self.root.after_cancel(self.after_id)  # Cancel the previous after call

        if self.on_break:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Break time! {countdown} seconds remaining")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.on_break = False
                self.reset_countdown_label.config(text="Resets in 60 seconds")
                self.hide_break_label()
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown
        else:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Resets in {countdown} seconds")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown


    

    def clear_video_feed(self):
        self.canvas_video.delete("all")
        self.canvas_video.configure(bg='white')
        self.canvas_video.create_text(
            self.canvas_video.winfo_width() // 2, self.canvas_video.winfo_height() // 2,
            text="Video feed stopped", font=("Arial", 20), fill="black"
        )

    def start_updates(self):
        if not self.is_tracking:
            self.start_eye_tracking()
        self.update()

    def update(self):
        ret, frame = self.vid.read()
        if ret and self.is_tracking:
            frame = cv2.flip(frame, 1)
            canvas_width = self.canvas_video.winfo_width()
            canvas_height = self.canvas_video.winfo_height()

            if canvas_width > 0 and canvas_height > 0:
                frame = self.resize_with_aspect_ratio(frame, width=canvas_width, height=canvas_height)
                if frame is not None:
                    frame = self.detect_eyes(frame)
                    self.photo = ImageTk.PhotoImage(image=Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                    self.canvas_video.create_image((canvas_width - self.photo.width()) // 2, (canvas_height - self.photo.height()) // 2, image=self.photo, anchor=tk.NW)
        self.root.after(self.delay, self.update)


    def resize_with_aspect_ratio(self, image, width=None, height=None, inter=cv2.INTER_AREA):
        (h, w) = image.shape[:2]

        # Calculate the aspect ratio of the image and the desired aspect ratio
        image_aspect = w / h
        desired_aspect = width / height if width is not None and height is not None else image_aspect

        # Calculate scaling factors for resizing the image while maintaining aspect ratio
        if image_aspect > desired_aspect:
            # Image is wider than the desired aspect ratio
            r = width / float(w)
            dim = (width, int(h * r))
        else:
            # Image is taller or equal to the desired aspect ratio
            r = height / float(h)
            dim = (int(w * r), height)

        # Check that dimensions are valid before attempting to resize
        if dim[0] > 0 and dim[1] > 0:
            resized = cv2.resize(image, dim, interpolation=inter)
            return resized
        return None




    def detect_eyes(self, frame):
        RIGHT_EYE_INDICES =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]
        LEFT_EYE_INDICES =[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]
        # Convert the frame to RGB for MediaPipe processing
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_frame)

        # Check if any face landmarks were detected
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Process each eye
                for eye_indices in [RIGHT_EYE_INDICES, LEFT_EYE_INDICES]:
                    # Calculate the bounding box for the eye
                    eye_coords = [(int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])) 
                                for idx in eye_indices 
                                for landmark in [face_landmarks.landmark[idx]]]
                    if eye_coords:
                        # Calculate the bounding box around the eye landmarks
                        x1, y1 = min([coord[0] for coord in eye_coords]), min([coord[1] for coord in eye_coords])
                        x2, y2 = max([coord[0] for coord in eye_coords]), max([coord[1] for coord in eye_coords])
                        eye_roi = frame[y1:y2, x1:x2]

                        # Apply Haar Cascade to detect blinks within the ROI
                        eyes = self.eye_cascade.detectMultiScale(eye_roi)
                        for (ex, ey, ew, eh) in eyes:
                            # Draw rectangles around detected eyes for visualization
                            cv2.rectangle(eye_roi, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

        return frame




    def set_strictness(self):
        value = self.strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 75:
                self.strictness = intvalue
                self.strictness_value.config(text=" Blink Strictness: " + str(intvalue))
                self.warning_msg.config(text="")
            else:
                self.warning_msg.config(text="Invalid Input! Please enter a number between 0 and 75!")
        except ValueError:
            self.warning_msg.config(text="Invalid Input! Please enter an integer!")

   
    def on_click(self, x, y, button, pressed):
        if pressed:
            self.total_click_amount += 1
            self.update_click_count()
            self.update_total_inputs_label()

    def on_press(self, key):
        self.total_keystroke_count += 1
        self.update_keystroke_count()
        self.update_total_inputs_label()


    def update_click_count(self):
        self.total_clicks.config(text="Total Clicks: " + str(self.total_click_amount))
        self.update_total_inputs_label()

    def update_keystroke_count(self):
        self.total_keystrokes_label.config(text="Total Keystroke Count: " + str(self.total_keystroke_count))
    def run_mouse_listener(self):
        with MouseListener(on_click=self.on_click) as listener:
            listener.join()

    # Separate method to run the keyboard listener
    def run_keyboard_listener(self):
        with KeyboardListener(on_press=self.on_press) as listener:
            listener.join()

    """
        Eye blink engine time!
    """

    def update_blink_count(self):
        if not self.on_break:  # Only update blink count if not on a break
            self.total_blink_count.config(text=f"Total Blink Count: {self.blink_count}")

            if self.blink_count >= self.strictness:
                self.initiate_break()

    def initiate_break(self):
        # Set the state to break and update the UI accordingly
        self.on_break = True
        self.show_break_label()
        self.reset_countdown_label.config(text="Timer stopped, take a break!")
        self.handle_reset_countdown(30)  # Start a 30-second break countdown

        # Reset all counts
        self.reset_counters()
    

app = EyeTrackingApp("MediaPipe Eye Tracking with Tkinter")
When I run the program, there are no dots nor rectangular boxes on screen. Try to fix that. No errors appear. 
ChatGPT
Based on your description, it seems like the eye detection and landmark visualization are not functioning as expected in your application. Let's address this by focusing on the detect_eyes method to ensure the landmarks and rectangles are properly drawn.

First, ensure that your detect_eyes method is correctly processing the landmarks and drawing both the landmarks and the rectangles around detected eyes. Here's an updated version of the method with some added comments and potential fixes:

python
Copy code
def detect_eyes(self, frame):
    RIGHT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
    LEFT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]

    # Convert the frame to RGB for MediaPipe processing
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = self.face_mesh.process(rgb_frame)

    # Check if any face landmarks were detected
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            for eye_indices in [RIGHT_EYE_INDICES, LEFT_EYE_INDICES]:
                # Calculate the bounding box for the eye
                eye_coords = [(int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0]))
                              for idx in eye_indices
                              for landmark in [face_landmarks.landmark[idx]]]

                if eye_coords:
                    x1, y1 = min([coord[0] for coord in eye_coords]), min([coord[1] for coord in eye_coords])
                    x2, y2 = max([coord[0] for coord in eye_coords]), max([coord[1] for coord in eye_coords])

                    # Draw landmarks for visualization
                    for (x, y) in eye_coords:
                        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)

                    # Define ROI for blink detection
                    eye_roi = frame[y1:y2, x1:x2]

                    # Apply Haar Cascade to detect blinks within the ROI
                    eyes = self.eye_cascade.detectMultiScale(eye_roi, scaleFactor=1.1, minNeighbors=4)
                    for (ex, ey, ew, eh) in eyes:
                        cv2.rectangle(eye_roi, (ex, ey), (ex + ew, ey + eh), (255, 0, 0), 2)

    return frame
In this updated method:

I've added a step to draw each landmark as a small circle on the frame. This will help you to visually confirm that MediaPipe is correctly identifying the eye landmarks.

The ROI for blink detection is defined based on the calculated bounding box of each eye's landmarks.

The Haar Cascade detector is applied within this ROI. I've added parameters scaleFactor and minNeighbors to the detectMultiScale method, which are often needed for fine-tuning the detection process. You may need to adjust these parameters based on your specific requirements.

Rectangles are drawn around any detected eyes within the ROI.

Please replace the detect_eyes method in your code with this updated version and test the application again. This should give you visual feedback on the screen regarding the landmarks and detected eyes, helping to diagnose any issues with the detection process. If you still encounter problems, further investigation into the specific configurations and conditions of your application environment might be necessary.

User
okay, the mediapipe green indices are being drawn. However, the rectangular boxes from eye cascade aren;t there, no errors in console. Try to fix that. 
ChatGPT
If the landmarks from MediaPipe are being drawn correctly but the rectangles from the Haar Cascade eye detector aren't appearing, it's possible that the eye detection isn't working as expected. This could be due to a variety of reasons, such as the size of the ROI being too small, the parameters for the detectMultiScale method not being optimal for your setting, or the Haar Cascade not performing well under the given conditions.

Here are some steps to troubleshoot and potentially fix this issue:

1. Adjust Haar Cascade Parameters
You can tweak the parameters of the detectMultiScale method. The scaleFactor and minNeighbors parameters are particularly important for detection quality:

python
Copy code
eyes = self.eye_cascade.detectMultiScale(eye_roi, scaleFactor=1.1, minNeighbors=5)
Experiment with different values for these parameters. A higher minNeighbors might reduce false positives, and a different scaleFactor can help in detecting eyes at various scales.

2. Check ROI Size
Ensure that the ROI is not too small. If the ROI around the eyes is too small, the Haar Cascade might not have enough information to detect the eyes. You can print out the dimensions of the ROI to check their size:

python
Copy code
print("ROI Size: ", eye_roi.shape)
If the ROI is too small, consider increasing the area around the landmarks used to define the ROI.

3. Visualize the ROI
To ensure that the ROI is correctly placed over the eyes, you can draw a rectangle around the ROI on the original frame:

python
Copy code
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Draw ROI rectangle
This visualization will help confirm that the ROI includes the eye area.

4. Adjust ROI Positioning
If the ROI is not correctly positioned over the eyes, you may need to adjust how you calculate the coordinates x1, y1, x2, y2 from the landmarks.

5. Test Haar Cascade Separately
As a sanity check, try using the Haar Cascade on the full frame or a manually defined ROI to ensure that it's capable of detecting eyes in your environment:

python
Copy code
eyes = self.eye_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5)
for (ex, ey, ew, eh) in eyes:
    cv2.rectangle(frame, (ex, ey), (ex + ew, ey + eh), (255, 0, 0), 2)
If this works, it suggests that the issue might be with how the ROIs are being defined or with the interaction between MediaPipe and the Haar Cascade.

6. Check Haar Cascade File
Ensure that the path to the Haar Cascade XML file is correct and that the file is being loaded properly.

Final Thoughts
It's important to note that Haar Cascades can sometimes struggle with real-time detection in varied lighting conditions or angles. If you continue to face issues, you might want to explore alternative methods for blink detection or consider using a more robust eye detection model.

User
use the ROI visualzer method
ChatGPT
To visualize the Regions of Interest (ROIs) that you're using for the Haar Cascade eye detection, you can draw rectangles around these areas on the original frame. This will help you understand if the ROIs are correctly positioned over the eyes. Here's how you can modify the detect_eyes method to include ROI visualization:

python
Copy code
def detect_eyes(self, frame):
    RIGHT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
    LEFT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]

    # Convert the frame to RGB for MediaPipe processing
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = self.face_mesh.process(rgb_frame)

    # Check if any face landmarks were detected
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            for eye_indices in [RIGHT_EYE_INDICES, LEFT_EYE_INDICES]:
                # Calculate the bounding box for the eye
                eye_coords = [(int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0]))
                              for idx in eye_indices 
                              for landmark in [face_landmarks.landmark[idx]]]
                if eye_coords:
                    x1, y1 = min([coord[0] for coord in eye_coords]), min([coord[1] for coord in eye_coords])
                    x2, y2 = max([coord[0] for coord in eye_coords]), max([coord[1] for coord in eye_coords])

                    # Draw a rectangle around the eye region for visualization
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)

                    # Define ROI for blink detection
                    eye_roi = frame[y1:y2, x1:x2]

                    # Apply Haar Cascade to detect blinks within the ROI
                    eyes = self.eye_cascade.detectMultiScale(eye_roi, scaleFactor=1.1, minNeighbors=5)
                    for (ex, ey, ew, eh) in eyes:
                        # Draw rectangles around detected eyes for visualization
                        cv2.rectangle(eye_roi, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

    return frame
In this updated method:

After calculating the coordinates for the bounding box of each eye (x1, y1, x2, y2), a rectangle is drawn around the eye region on the original frame using cv2.rectangle. This rectangle is drawn in blue (255, 0, 0) for visibility.

The ROI (eye_roi) is then extracted and used for the Haar Cascade detection as before.

Detected eyes within the ROI are marked with green rectangles.

By running your application with this updated method, you'll be able to see both the region where the application is looking for eyes (blue rectangle) and the actual detected eyes (green rectangles). This visualization will help you adjust the parameters or ROI calculation if necessary.

User
here is my code: 
import time
import threading
import cv2
import mediapipe as mp
import tkinter as tk
from PIL import Image, ImageTk
from pynput.mouse import Listener as MouseListener
from pynput.keyboard import Listener as KeyboardListener, Key
from tkinter import ttk



class EyeTrackingApp:
    def __init__(self, window_title):
        self.root = tk.Tk()
        self.root.state('zoomed')
        self.root.title(window_title)
        self.root.configure(bg='#404040')

        self.on_break = False


        # Create a style object
        style = ttk.Style()
        style.theme_use('clam')  # Use the 'clam' theme as a base for customization

        # Configure style for Button
        style.configure('Custom.TButton', 
                        background='#404040', 
                        foreground='white', 
                        bordercolor='white', 
                        borderwidth=2,
                        font=('Segoe UI', 14))

        # Configure style for Text
        text_bg = '#404040'  # Dark grey background for text
        text_fg = 'white'    # White text color

        # Main Frame
        main_frame = tk.Frame(self.root, bg='#404040')
        main_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Video Frame
        video_frame = tk.Frame(main_frame, bg='#404040')
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # Right Section Frame
        right_frame = tk.Frame(main_frame, bg='#404040')
        right_frame.pack(side=tk.RIGHT, fill=tk.Y)

        self.canvas_video = tk.Canvas(video_frame, bg='#404040')
        self.canvas_video.pack(fill="both", expand=True)

        # Time Elapsed Frame
        self.total_time_count = 0
        time_frame = tk.Frame(right_frame, bg='#404040')
        time_frame.pack(side=tk.TOP, fill=tk.X, pady=5)
        self.total_time_elapsed = tk.Label(time_frame, text="Total Time Elapsed: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_time_elapsed.pack()
        self.change_total_time_count()

        # Strictness Frame
        self.strictness = 10
        strictness_frame = tk.Frame(right_frame, bg='#404040')
        strictness_frame.pack(side=tk.TOP, fill=tk.X)
        strictness_frame.columnconfigure(0, weight=1)
        strictness_frame.columnconfigure(1, weight=1)
        self.strictness_value = tk.Label(strictness_frame, font=("Segoe UI", 20), text='Blink Strictness: ' + str(self.strictness), fg='white', bg='#404040')
        self.strictness_value.grid(row=0, columnspan=2)
        self.strictness_textbox = tk.Text(strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_strictness_button = ttk.Button(strictness_frame, text="Set Strictness", style='Custom.TButton', command=self.set_strictness)
        self.set_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.strictness_explanation = tk.Label(strictness_frame, text="Blink Strictness means the maximum blink count per minute", font=("Arial", 15), fg='white', bg='#404040')
        self.strictness_explanation.grid(row=3, columnspan=2)
        self.warning_msg = tk.Label(strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.warning_msg.grid(row=2, columnspan=2)

        # Blink Count Frame
        self.blink_count = 0
        self.EAR_THRESHOLD = 0.21
        self.eye_closed = False
        self.blink_count_frame = tk.Frame(right_frame, bg='#404040')
        self.blink_count_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_blink_count = tk.Label(self.blink_count_frame, text="Total Blink Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_blink_count.pack()

        # Break Label
        self.break_label = tk.Label(right_frame, text="Time for a break!", font=("Segoe UI", 20), fg='red', bg='#404040')
        self.break_label.pack(pady=10)
        self.break_label.pack_forget()

        # Spacer Frame
        spacer_frame = tk.Frame(right_frame, height=20, bg='#404040')
        spacer_frame.pack(side=tk.TOP, fill=tk.X)

        # Clicks and Keystrokes Frame
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        clicks_frame = tk.Frame(right_frame, bg='#404040')
        clicks_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_clicks = tk.Label(clicks_frame, text="Total Clicks: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_clicks.pack()
        keystrokes_frame = tk.Frame(right_frame, bg='#404040')
        keystrokes_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_keystrokes_label = tk.Label(keystrokes_frame, text="Total Keystroke Count: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_keystrokes_label.pack()

        # Total Inputs Frame
        total_inputs_frame = tk.Frame(right_frame, bg='#404040')
        total_inputs_frame.pack(side=tk.TOP, fill=tk.X)
        self.total_inputs_label = tk.Label(total_inputs_frame, text="Total Inputs: 0", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.total_inputs_label.pack()

        # Input Strictness Frame
        self.input_strictness = 50
        input_strictness_frame = tk.Frame(right_frame, bg='#404040')
        input_strictness_frame.pack(side=tk.TOP, fill=tk.X)
        input_strictness_frame.columnconfigure(0, weight=1)
        input_strictness_frame.columnconfigure(1, weight=1)
        self.input_strictness_value = tk.Label(input_strictness_frame, font=("Segoe UI", 20), text='Input Strictness: ' + str(self.input_strictness), fg='white', bg='#404040')
        self.input_strictness_value.grid(row=0, columnspan=2)
        self.input_strictness_textbox = tk.Text(input_strictness_frame, font=("Segoe UI", 20), height=1, width=5, bg=text_bg, fg=text_fg)
        self.input_strictness_textbox.grid(row=1, column=0, pady=(0, 5), padx=5, sticky="ew")
        self.set_input_strictness_button = ttk.Button(input_strictness_frame, text="Set Input Strictness", style='Custom.TButton', command=self.set_input_strictness)
        self.set_input_strictness_button.grid(row=1, column=1, pady=(0, 5), padx=5, sticky="ew")
        self.input_strictness_warning_msg = tk.Label(input_strictness_frame, text="", font=("Arial", 15), fg='red', bg='#404040')
        self.input_strictness_warning_msg.grid(row=2, columnspan=2)

        # Reset Countdown Label
        self.reset_countdown_label = tk.Label(right_frame, text="Resets in 60 seconds", font=("Segoe UI", 20), fg='white', bg='#404040')
        self.reset_countdown_label.pack(side=tk.TOP, fill=tk.X)

        # Input Listener Thread
        input_listener_thread = threading.Thread(target=self.run_input_listeners)
        input_listener_thread.daemon = True
        input_listener_thread.start()

        self.is_tracking = False
        self.video_source = 0
        self.vid = cv2.VideoCapture(self.video_source)
        self.delay = 15

        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)

        # Initialize Haar Cascade for eye blink detection
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        


        # Start video updates
        self.root.after(100, self.start_updates)
        self.handle_reset_countdown()
        self.root.mainloop()
    def set_input_strictness(self):
        value = self.input_strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 500:
                self.input_strictness = intvalue
                self.input_strictness_value.config(text="Input Strictness: " + str(intvalue))
                self.input_strictness_warning_msg.config(text="")
            else:
                self.input_strictness_warning_msg.config(text="Invalid Input! Please enter a number between 0 and 500!")
        except ValueError:
            self.input_strictness_warning_msg.config(text="Invalid Input! Please enter an integer!")

    def reset_counters(self):
        # Reset all the counters and update their respective labels
        self.blink_count = 0
        self.total_blink_count.config(text="Total Blink Count: 0")
        self.total_click_amount = 0
        self.total_keystroke_count = 0
        self.update_click_count()
        self.update_keystroke_count()
        self.update_total_inputs_label()

    def change_total_time_count(self):
        self.total_time_elapsed.config(text=f"Total Time Elapsed: {self.total_time_count}")
        self.total_time_count += 1
        self.root.after(1000, self.change_total_time_count)

    def update_total_inputs_label(self):
        total_inputs = self.total_click_amount + self.total_keystroke_count
        self.total_inputs_label.config(text=f"Total Inputs: {total_inputs}")
        if total_inputs >= self.input_strictness and not self.on_break:
            self.initiate_break()


    def show_break_label(self):
        self.break_label.pack()

    def hide_break_label(self):
        self.break_label.pack_forget()

    def run_input_listeners(self):
        # Initialize the mouse listener
        mouse_listener = MouseListener(on_click=self.on_click)
        mouse_listener.start()

        # Initialize the keyboard listener
        keyboard_listener = KeyboardListener(on_press=self.on_press)
        keyboard_listener.start()


    def start_eye_tracking(self):
        # Directly start tracking without checking the button state
        self.is_tracking = True
        self.canvas_video.configure(bg='black')

    def handle_reset_countdown(self, countdown=60):
        if hasattr(self, 'after_id'):  # Check if there's an existing after call
            self.root.after_cancel(self.after_id)  # Cancel the previous after call

        if self.on_break:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Break time! {countdown} seconds remaining")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.on_break = False
                self.reset_countdown_label.config(text="Resets in 60 seconds")
                self.hide_break_label()
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown
        else:
            if countdown > 0:
                self.reset_countdown_label.config(text=f"Resets in {countdown} seconds")
                self.after_id = self.root.after(1000, self.handle_reset_countdown, countdown - 1)
            else:
                self.reset_counters()  # Call the method to reset the counters
                self.handle_reset_countdown()  # Restart the countdown


    

    def clear_video_feed(self):
        self.canvas_video.delete("all")
        self.canvas_video.configure(bg='white')
        self.canvas_video.create_text(
            self.canvas_video.winfo_width() // 2, self.canvas_video.winfo_height() // 2,
            text="Video feed stopped", font=("Arial", 20), fill="black"
        )

    def start_updates(self):
        if not self.is_tracking:
            self.start_eye_tracking()
        self.update()

    def update(self):
        ret, frame = self.vid.read()
        if ret and self.is_tracking:
            frame = cv2.flip(frame, 1)
            canvas_width = self.canvas_video.winfo_width()
            canvas_height = self.canvas_video.winfo_height()

            if canvas_width > 0 and canvas_height > 0:
                frame = self.resize_with_aspect_ratio(frame, width=canvas_width, height=canvas_height)
                if frame is not None:
                    frame = self.detect_eyes(frame)
                    self.photo = ImageTk.PhotoImage(image=Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                    self.canvas_video.create_image((canvas_width - self.photo.width()) // 2, (canvas_height - self.photo.height()) // 2, image=self.photo, anchor=tk.NW)
        self.root.after(self.delay, self.update)


    def resize_with_aspect_ratio(self, image, width=None, height=None, inter=cv2.INTER_AREA):
        (h, w) = image.shape[:2]

        # Calculate the aspect ratio of the image and the desired aspect ratio
        image_aspect = w / h
        desired_aspect = width / height if width is not None and height is not None else image_aspect

        # Calculate scaling factors for resizing the image while maintaining aspect ratio
        if image_aspect > desired_aspect:
            # Image is wider than the desired aspect ratio
            r = width / float(w)
            dim = (width, int(h * r))
        else:
            # Image is taller or equal to the desired aspect ratio
            r = height / float(h)
            dim = (int(w * r), height)

        # Check that dimensions are valid before attempting to resize
        if dim[0] > 0 and dim[1] > 0:
            resized = cv2.resize(image, dim, interpolation=inter)
            return resized
        return None




    def detect_eyes(self, frame):
        RIGHT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
        LEFT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]

        # Convert the frame to RGB for MediaPipe processing
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_frame)

        # Check if any face landmarks were detected
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                for eye_indices in [RIGHT_EYE_INDICES, LEFT_EYE_INDICES]:
                    # Calculate the bounding box for the eye
                    eye_coords = [(int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0]))
                                for idx in eye_indices 
                                for landmark in [face_landmarks.landmark[idx]]]
                    if eye_coords:
                        x1, y1 = min([coord[0] for coord in eye_coords]), min([coord[1] for coord in eye_coords])
                        x2, y2 = max([coord[0] for coord in eye_coords]), max([coord[1] for coord in eye_coords])

                        # Draw a rectangle around the eye region for visualization
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)

                        # Define ROI for blink detection
                        eye_roi = frame[y1:y2, x1:x2]

                        # Apply Haar Cascade to detect blinks within the ROI
                        eyes = self.eye_cascade.detectMultiScale(eye_roi, scaleFactor=1.1, minNeighbors=5)
                        for (ex, ey, ew, eh) in eyes:
                            # Draw rectangles around detected eyes for visualization
                            cv2.rectangle(eye_roi, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

        return frame






    def set_strictness(self):
        value = self.strictness_textbox.get("1.0", "end").strip()
        try:
            intvalue = int(value)
            if 0 <= intvalue <= 75:
                self.strictness = intvalue
                self.strictness_value.config(text=" Blink Strictness: " + str(intvalue))
                self.warning_msg.config(text="")
            else:
                self.warning_msg.config(text="Invalid Input! Please enter a number between 0 and 75!")
        except ValueError:
            self.warning_msg.config(text="Invalid Input! Please enter an integer!")

   
    def on_click(self, x, y, button, pressed):
        if pressed:
            self.total_click_amount += 1
            self.update_click_count()
            self.update_total_inputs_label()

    def on_press(self, key):
        self.total_keystroke_count += 1
        self.update_keystroke_count()
        self.update_total_inputs_label()


    def update_click_count(self):
        self.total_clicks.config(text="Total Clicks: " + str(self.total_click_amount))
        self.update_total_inputs_label()

    def update_keystroke_count(self):
        self.total_keystrokes_label.config(text="Total Keystroke Count: " + str(self.total_keystroke_count))
    def run_mouse_listener(self):
        with MouseListener(on_click=self.on_click) as listener:
            listener.join()

    # Separate method to run the keyboard listener
    def run_keyboard_listener(self):
        with KeyboardListener(on_press=self.on_press) as listener:
            listener.join()

    """
        Eye blink engine time!
    """

    def update_blink_count(self):
        if not self.on_break:  # Only update blink count if not on a break
            self.total_blink_count.config(text=f"Total Blink Count: {self.blink_count}")

            if self.blink_count >= self.strictness:
                self.initiate_break()

    def initiate_break(self):
        # Set the state to break and update the UI accordingly
        self.on_break = True
        self.show_break_label()
        self.reset_countdown_label.config(text="Timer stopped, take a break!")
        self.handle_reset_countdown(30)  # Start a 30-second break countdown

        # Reset all counts
        self.reset_counters()
    

app = EyeTrackingApp("MediaPipe Eye Tracking with Tkinter")
Ok, i see the blue box, and i dont see the eye indices anymore. It is prefectly there on my eye and not anywhere else on the screen. Now, make the code pickup my blinks. Right now, when I blink, the code doesn't pick it up. 
ChatGPT
To improve the detection of blinks in your detect_eyes method, you need to refine the Haar Cascade logic within the region of interest (ROI) defined by the blue box around the eyes. Here are a few suggestions to enhance blink detection:

Adjust Haar Cascade Parameters: Fine-tune the scaleFactor and minNeighbors parameters in the detectMultiScale function to improve detection sensitivity.

Check Eye ROI Size: Ensure the ROI is not too small, as a very small region might not provide enough information for the Haar Cascade to detect eyes effectively.

Blink Detection Logic: Implement a mechanism to detect blinks based on the absence of detected eyes in consecutive frames or based on changes in the eye aspect ratio.

Here's an updated version of your detect_eyes method with some adjustments for better blink detection:

python
Copy code
def detect_eyes(self, frame):
    # Eye indices for MediaPipe
    RIGHT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
    LEFT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]

    # Convert the frame to RGB for MediaPipe processing
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = self.face_mesh.process(rgb_frame)

    # Initialize variables
    eyes_detected = False

    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            for eye_indices in [RIGHT_EYE_INDICES, LEFT_EYE_INDICES]:
                eye_coords = [(int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0]))
                              for idx in eye_indices 
                              for landmark in [face_landmarks.landmark[idx]]]
                if eye_coords:
                    x1, y1 = min([coord[0] for coord in eye_coords]), min([coord[1] for coord in eye_coords])
                    x2, y2 = max([coord[0] for coord in eye_coords]), max([coord[1] for coord in eye_coords])

                    # Draw a rectangle around the eye region for visualization
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)

                    # Define ROI for blink detection
                    eye_roi = frame[y1:y2, x1:x2]

                    # Apply Haar Cascade to detect blinks within the ROI
                    eyes = self.eye_cascade.detectMultiScale(eye_roi, scaleFactor=1.1, minNeighbors=5)
                    for (ex, ey, ew, eh) in eyes:
                        eyes_detected = True
                        cv2.rectangle(eye_roi, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

    # Blink detection logic
    if not eyes_detected:
        # Eyes not detected in this frame (potential blink)
        if not self.eye_closed:
            self.eye_closed = True
            self.blink_count += 1
            self.update_blink_count()
    else:
        self.eye_closed = False

    return frame
In this version:

The eyes_detected flag tracks if eyes are detected within the ROI.

If no eyes are detected (eyes_detected is False), and if the eyes were previously open (self.eye_closed is False), it's considered a blink. The blink count is updated accordingly.

The self.eye_closed variable is reset when eyes are detected again.

Remember, accurate blink detection can be sensitive to various factors, including lighting conditions, camera quality, and the user's distance from the camera. You may need to experiment with the Haar Cascade parameters and the conditions for blink detection to optimize performance for your specific setup.
